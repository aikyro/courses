{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Agent Evaluation Workshop**\n",
    "\n",
    "**Duration:** ~50 minutes  \n",
    "**Prerequisites:** Familiarity with LLM tool-calling, basic Python  \n",
    "**API:** Google Generative AI (Gemini)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to evaluate individual agent components (tool selection, parameter extraction)\n",
    "- How to use an LLM as a judge to score agent outputs\n",
    "- How to systematically improve agent performance through prompt iteration\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Structure\n",
    "\n",
    "| Exercise | Topic | Duration |\n",
    "|----------|-------|----------|\n",
    "| 1 | Component-Level Evaluation | 20 min |\n",
    "| 2 | LLM-as-Judge Evaluation | 15 min |\n",
    "| 3 | Prompt Iteration & Improvement | 15 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your Google API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from getpass import getpass \n",
    "\n",
    "api_key = getpass(\"Paste your Google API key: \").strip()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Component-Level Evaluation (20 min)\n",
    "\n",
    "**Goal:** Measure how well individual agent components work — tool selection and parameter extraction.\n",
    "\n",
    "We'll build a simple agent with 4 tools, then run a structured test suite against it to measure:\n",
    "- **Tool Selection Accuracy** — does the agent pick the right tool?\n",
    "- **Parameter Extraction Accuracy** — does it extract the correct arguments?\n",
    "\n",
    "```\n",
    "              ┌─────────────┐\n",
    "  Query ───> │  LLM Agent  │ ───> Tool Call + Params\n",
    "              └─────────────┘\n",
    "                    │\n",
    "                    ▼\n",
    "           Compare against expected\n",
    "           tool + expected params\n",
    "                    │\n",
    "                    ▼\n",
    "             Accuracy Scores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Agent Tools\n",
    "\n",
    "We create 4 tools that our agent can call. Each tool has a clear docstring — this is what the LLM reads to decide which tool to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: ['get_weather', 'search_web', 'calculate', 'get_news']\n"
     ]
    }
   ],
   "source": [
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the current weather for a given city.\n",
    "\n",
    "    Use this tool when the user asks about weather conditions,\n",
    "    temperature, or forecast for a specific city or location.\n",
    "\n",
    "    Args:\n",
    "        city: Name of the city (e.g., \"Delhi\", \"London\", \"Tokyo\")\n",
    "\n",
    "    Returns:\n",
    "        dict with city, temperature, and conditions\n",
    "    \"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"delhi\": {\"temp\": 35, \"conditions\": \"Sunny\"},\n",
    "        \"london\": {\"temp\": 12, \"conditions\": \"Cloudy\"},\n",
    "        \"tokyo\": {\"temp\": 22, \"conditions\": \"Clear\"},\n",
    "        \"new york\": {\"temp\": 18, \"conditions\": \"Partly Cloudy\"},\n",
    "        \"mumbai\": {\"temp\": 32, \"conditions\": \"Humid\"},\n",
    "    }\n",
    "    data = weather_data.get(city.lower(), {\"temp\": 20, \"conditions\": \"Unknown\"})\n",
    "    return {\"city\": city, \"temperature_c\": data[\"temp\"], \"conditions\": data[\"conditions\"], \"status\": \"success\"}\n",
    "\n",
    "\n",
    "def search_web(query: str) -> dict:\n",
    "    \"\"\"Search the web for information on a given topic.\n",
    "\n",
    "    Use this tool when the user asks general knowledge questions,\n",
    "    wants to find information about a topic, or needs facts that\n",
    "    aren't covered by other specialized tools.\n",
    "\n",
    "    Args:\n",
    "        query: The search query string\n",
    "\n",
    "    Returns:\n",
    "        dict with search results summary\n",
    "    \"\"\"\n",
    "    return {\"query\": query, \"results\": f\"Top results for: {query}\", \"status\": \"success\"}\n",
    "\n",
    "\n",
    "def calculate(expression: str) -> dict:\n",
    "    \"\"\"Evaluate a mathematical expression and return the result.\n",
    "\n",
    "    Use this tool when the user asks to compute, calculate, or\n",
    "    solve a math problem. Supports basic arithmetic (+, -, *, /),\n",
    "    powers (**), and parentheses.\n",
    "\n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 3 * 4\")\n",
    "\n",
    "    Returns:\n",
    "        dict with the expression and its computed result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return {\"expression\": expression, \"result\": result, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"expression\": expression, \"error\": str(e), \"status\": \"error\"}\n",
    "\n",
    "\n",
    "def get_news(topic: str) -> dict:\n",
    "    \"\"\"Get the latest news headlines about a specific topic.\n",
    "\n",
    "    Use this tool when the user asks about recent news, current events,\n",
    "    or headlines related to a specific topic or subject.\n",
    "\n",
    "    Args:\n",
    "        topic: The news topic to search for (e.g., \"AI\", \"sports\", \"politics\")\n",
    "\n",
    "    Returns:\n",
    "        dict with news headlines for the topic\n",
    "    \"\"\"\n",
    "    return {\"topic\": topic, \"headlines\": [f\"Breaking: Latest on {topic}\"], \"status\": \"success\"}\n",
    "\n",
    "\n",
    "# Map tool names to functions for execution\n",
    "TOOLS = [get_weather, search_web, calculate, get_news]\n",
    "TOOL_MAP = {fn.__name__: fn for fn in TOOLS}\n",
    "\n",
    "print(\"Available tools:\", list(TOOL_MAP.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Agent Caller\n",
    "\n",
    "This function sends a query to the LLM with tool definitions and returns which tool the LLM chose and what parameters it extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with access to tools.\n",
    "When the user asks a question, decide which tool to call and extract the correct parameters.\n",
    "Always use a tool to answer — do not answer from memory.\"\"\"\n",
    "\n",
    "\n",
    "def call_agent(query: str, system_prompt: str = SYSTEM_PROMPT) -> dict:\n",
    "    \"\"\"Send a query to the agent and return the tool call it makes.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'tool_name' and 'params', or 'error' if no tool was called\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=query,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=system_prompt,\n",
    "                tools=TOOLS,\n",
    "                tool_config=types.ToolConfig(\n",
    "                    function_calling_config=types.FunctionCallingConfig(mode=\"ANY\")\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Extract the function call from the response\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if part.function_call:\n",
    "                return {\n",
    "                    \"tool_name\": part.function_call.name,\n",
    "                    \"params\": dict(part.function_call.args),\n",
    "                }\n",
    "\n",
    "        return {\"tool_name\": None, \"params\": {}, \"error\": \"No tool call in response\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"tool_name\": None, \"params\": {}, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Test Cases\n",
    "\n",
    "20 test queries covering all 4 tools — each with expected tool and expected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test cases: 20\n",
      "Tools covered: {'calculate', 'get_news', 'search_web', 'get_weather'}\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    # --- get_weather (5 cases) ---\n",
    "    {\"query\": \"Weather in Delhi\", \"expected_tool\": \"get_weather\", \"expected_params\": {\"city\": \"Delhi\"}},\n",
    "    {\"query\": \"What's the temperature in London?\", \"expected_tool\": \"get_weather\", \"expected_params\": {\"city\": \"London\"}},\n",
    "    {\"query\": \"Is it raining in Tokyo right now?\", \"expected_tool\": \"get_weather\", \"expected_params\": {\"city\": \"Tokyo\"}},\n",
    "    {\"query\": \"Tell me the weather forecast for Mumbai\", \"expected_tool\": \"get_weather\", \"expected_params\": {\"city\": \"Mumbai\"}},\n",
    "    {\"query\": \"How hot is it in New York?\", \"expected_tool\": \"get_weather\", \"expected_params\": {\"city\": \"New York\"}},\n",
    "\n",
    "    # --- search_web (5 cases) ---\n",
    "    {\"query\": \"Who invented the telephone?\", \"expected_tool\": \"search_web\", \"expected_params\": {\"query\": \"Who invented the telephone?\"}},\n",
    "    {\"query\": \"What is photosynthesis?\", \"expected_tool\": \"search_web\", \"expected_params\": {\"query\": \"What is photosynthesis?\"}},\n",
    "    {\"query\": \"Tell me about the history of chess\", \"expected_tool\": \"search_web\", \"expected_params\": {\"query\": \"history of chess\"}},\n",
    "    {\"query\": \"How does a combustion engine work?\", \"expected_tool\": \"search_web\", \"expected_params\": {\"query\": \"How does a combustion engine work?\"}},\n",
    "    {\"query\": \"What are the benefits of meditation?\", \"expected_tool\": \"search_web\", \"expected_params\": {\"query\": \"benefits of meditation\"}},\n",
    "\n",
    "    # --- calculate (5 cases) ---\n",
    "    {\"query\": \"What is 25 * 4?\", \"expected_tool\": \"calculate\", \"expected_params\": {\"expression\": \"25 * 4\"}},\n",
    "    {\"query\": \"Calculate 100 / 3\", \"expected_tool\": \"calculate\", \"expected_params\": {\"expression\": \"100 / 3\"}},\n",
    "    {\"query\": \"What's 2 to the power of 10?\", \"expected_tool\": \"calculate\", \"expected_params\": {\"expression\": \"2 ** 10\"}},\n",
    "    {\"query\": \"Compute (15 + 25) * 2\", \"expected_tool\": \"calculate\", \"expected_params\": {\"expression\": \"(15 + 25) * 2\"}},\n",
    "    {\"query\": \"How much is 999 - 123?\", \"expected_tool\": \"calculate\", \"expected_params\": {\"expression\": \"999 - 123\"}},\n",
    "\n",
    "    # --- get_news (5 cases) ---\n",
    "    {\"query\": \"What's the latest news about AI?\", \"expected_tool\": \"get_news\", \"expected_params\": {\"topic\": \"AI\"}},\n",
    "    {\"query\": \"Any recent sports headlines?\", \"expected_tool\": \"get_news\", \"expected_params\": {\"topic\": \"sports\"}},\n",
    "    {\"query\": \"Show me news about climate change\", \"expected_tool\": \"get_news\", \"expected_params\": {\"topic\": \"climate change\"}},\n",
    "    {\"query\": \"Latest updates on the stock market\", \"expected_tool\": \"get_news\", \"expected_params\": {\"topic\": \"stock market\"}},\n",
    "    {\"query\": \"What's happening in space exploration?\", \"expected_tool\": \"get_news\", \"expected_params\": {\"topic\": \"space exploration\"}},\n",
    "]\n",
    "\n",
    "print(f\"Total test cases: {len(test_cases)}\")\n",
    "print(f\"Tools covered: {set(tc['expected_tool'] for tc in test_cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Test Harness\n",
    "\n",
    "We send each query to the agent and compare:\n",
    "1. Did it select the **correct tool**?\n",
    "2. Did it extract the **correct parameters**?\n",
    "\n",
    "For parameters, we use a **fuzzy match** with three strategies:\n",
    "- **Case-insensitive** comparison\n",
    "- **Bidirectional containment** — either string containing the other (handles rephrasing)\n",
    "- **Whitespace normalization** — `2**10` matches `2 ** 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a string for fuzzy comparison: lowercase, strip, collapse whitespace.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).lower().strip())\n",
    "\n",
    "\n",
    "def fuzzy_param_match(expected_params: dict, actual_params: dict) -> bool:\n",
    "    \"\"\"Check if actual params match expected values.\n",
    "\n",
    "    Uses bidirectional containment after normalization:\n",
    "    - Case-insensitive\n",
    "    - Whitespace-normalized (so '2**10' matches '2 ** 10')\n",
    "    - Either string containing the other counts as a match\n",
    "    \"\"\"\n",
    "    for key, expected_val in expected_params.items():\n",
    "        actual_norm = normalize(actual_params.get(key, \"\"))\n",
    "        expected_norm = normalize(expected_val)\n",
    "        # Also compare with all whitespace removed (handles '2**10' vs '2 ** 10')\n",
    "        actual_compact = actual_norm.replace(\" \", \"\")\n",
    "        expected_compact = expected_norm.replace(\" \", \"\")\n",
    "        if (expected_norm in actual_norm\n",
    "            or actual_norm in expected_norm\n",
    "            or expected_compact == actual_compact):\n",
    "            continue\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def run_evaluation(test_cases: list, system_prompt: str = SYSTEM_PROMPT, label: str = \"Evaluation\") -> dict:\n",
    "    \"\"\"Run all test cases and return accuracy metrics.\"\"\"\n",
    "    results = []\n",
    "    tool_correct = 0\n",
    "    param_correct = 0\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    for i, tc in enumerate(test_cases):\n",
    "        result = call_agent(tc[\"query\"], system_prompt=system_prompt)\n",
    "\n",
    "        tool_match = result[\"tool_name\"] == tc[\"expected_tool\"]\n",
    "        param_match = fuzzy_param_match(tc[\"expected_params\"], result[\"params\"])\n",
    "\n",
    "        if tool_match:\n",
    "            tool_correct += 1\n",
    "        if param_match:\n",
    "            param_correct += 1\n",
    "\n",
    "        status = \"PASS\" if (tool_match and param_match) else \"FAIL\"\n",
    "        icon = \"+\" if status == \"PASS\" else \"X\"\n",
    "\n",
    "        print(f\"  [{icon}] {i+1:2d}. {tc['query'][:45]:<45}  \"\n",
    "              f\"Tool: {result['tool_name'] or 'None':<15} \"\n",
    "              f\"{'OK' if tool_match else 'WRONG':>5}  \"\n",
    "              f\"Params: {'OK' if param_match else 'WRONG'}\")\n",
    "\n",
    "        if not tool_match:\n",
    "            print(f\"        Expected tool: {tc['expected_tool']}\")\n",
    "        if not param_match:\n",
    "            print(f\"        Expected params: {tc['expected_params']}\")\n",
    "            print(f\"        Got params:      {result['params']}\")\n",
    "\n",
    "        results.append({\n",
    "            \"query\": tc[\"query\"],\n",
    "            \"expected_tool\": tc[\"expected_tool\"],\n",
    "            \"actual_tool\": result[\"tool_name\"],\n",
    "            \"tool_match\": tool_match,\n",
    "            \"param_match\": param_match,\n",
    "            \"expected_params\": tc[\"expected_params\"],\n",
    "            \"actual_params\": result[\"params\"],\n",
    "        })\n",
    "\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "\n",
    "    total = len(test_cases)\n",
    "    tool_accuracy = tool_correct / total * 100\n",
    "    param_accuracy = param_correct / total * 100\n",
    "    full_accuracy = sum(1 for r in results if r[\"tool_match\"] and r[\"param_match\"]) / total * 100\n",
    "\n",
    "    print(f\"\\n{'-' * 70}\")\n",
    "    print(f\"  RESULTS\")\n",
    "    print(f\"  Tool Selection Accuracy:    {tool_correct}/{total} = {tool_accuracy:.1f}%\")\n",
    "    print(f\"  Parameter Extraction:       {param_correct}/{total} = {param_accuracy:.1f}%\")\n",
    "    print(f\"  Full Match (tool + params): {sum(1 for r in results if r['tool_match'] and r['param_match'])}/{total} = {full_accuracy:.1f}%\")\n",
    "    print(f\"{'-' * 70}\")\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"tool_accuracy\": tool_accuracy,\n",
    "        \"param_accuracy\": param_accuracy,\n",
    "        \"full_accuracy\": full_accuracy,\n",
    "        \"results\": results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  Baseline Evaluation\n",
      "======================================================================\n",
      "  [+]  1. Weather in Delhi                               Tool: get_weather        OK  Params: OK\n",
      "  [+]  2. What's the temperature in London?              Tool: get_weather        OK  Params: OK\n",
      "  [+]  3. Is it raining in Tokyo right now?              Tool: get_weather        OK  Params: OK\n",
      "  [+]  4. Tell me the weather forecast for Mumbai        Tool: get_weather        OK  Params: OK\n",
      "  [+]  5. How hot is it in New York?                     Tool: get_weather        OK  Params: OK\n",
      "  [+]  6. Who invented the telephone?                    Tool: search_web         OK  Params: OK\n",
      "  [+]  7. What is photosynthesis?                        Tool: search_web         OK  Params: OK\n",
      "  [+]  8. Tell me about the history of chess             Tool: search_web         OK  Params: OK\n",
      "  [X]  9. How does a combustion engine work?             Tool: search_web         OK  Params: WRONG\n",
      "        Expected params: {'query': 'How does a combustion engine work?'}\n",
      "        Got params:      {'query': 'how a combustion engine works'}\n",
      "  [+] 10. What are the benefits of meditation?           Tool: search_web         OK  Params: OK\n",
      "  [+] 11. What is 25 * 4?                                Tool: calculate          OK  Params: OK\n",
      "  [+] 12. Calculate 100 / 3                              Tool: calculate          OK  Params: OK\n",
      "  [+] 13. What's 2 to the power of 10?                   Tool: calculate          OK  Params: OK\n",
      "  [+] 14. Compute (15 + 25) * 2                          Tool: calculate          OK  Params: OK\n",
      "  [+] 15. How much is 999 - 123?                         Tool: calculate          OK  Params: OK\n",
      "  [+] 16. What's the latest news about AI?               Tool: get_news           OK  Params: OK\n",
      "  [+] 17. Any recent sports headlines?                   Tool: get_news           OK  Params: OK\n",
      "  [+] 18. Show me news about climate change              Tool: get_news           OK  Params: OK\n",
      "  [+] 19. Latest updates on the stock market             Tool: get_news           OK  Params: OK\n",
      "  [+] 20. What's happening in space exploration?         Tool: get_news           OK  Params: OK\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  RESULTS\n",
      "  Tool Selection Accuracy:    20/20 = 100.0%\n",
      "  Parameter Extraction:       19/20 = 95.0%\n",
      "  Full Match (tool + params): 19/20 = 95.0%\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "baseline_eval = run_evaluation(test_cases, label=\"Baseline Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Per-Tool Breakdown\n",
    "\n",
    "Let's see which tools the agent handles well and which it struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool              Tool Acc  Param Acc   Full Acc\n",
      "--------------------------------------------------\n",
      "calculate             100%       100%       100%\n",
      "get_news              100%       100%       100%\n",
      "get_weather           100%       100%       100%\n",
      "search_web            100%        80%        80%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group results by expected tool\n",
    "tool_stats = defaultdict(lambda: {\"total\": 0, \"tool_ok\": 0, \"param_ok\": 0, \"full_ok\": 0})\n",
    "\n",
    "for r in baseline_eval[\"results\"]:\n",
    "    tool = r[\"expected_tool\"]\n",
    "    tool_stats[tool][\"total\"] += 1\n",
    "    if r[\"tool_match\"]:\n",
    "        tool_stats[tool][\"tool_ok\"] += 1\n",
    "    if r[\"param_match\"]:\n",
    "        tool_stats[tool][\"param_ok\"] += 1\n",
    "    if r[\"tool_match\"] and r[\"param_match\"]:\n",
    "        tool_stats[tool][\"full_ok\"] += 1\n",
    "\n",
    "print(f\"{'Tool':<15} {'Tool Acc':>10} {'Param Acc':>10} {'Full Acc':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for tool, stats in sorted(tool_stats.items()):\n",
    "    t = stats[\"total\"]\n",
    "    print(f\"{tool:<15} {stats['tool_ok']/t*100:>9.0f}% {stats['param_ok']/t*100:>9.0f}% {stats['full_ok']/t*100:>9.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Exercise 1\n",
    "\n",
    "- Which tool had the lowest accuracy? Why might that be?\n",
    "- Were parameter extraction errors caused by phrasing differences or actual mistakes?\n",
    "- How would you improve tool selection? (Hint: better docstrings, system prompt, or tool names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: LLM-as-Judge Evaluation (15 min)\n",
    "\n",
    "**Goal:** Use a second LLM call to evaluate the quality of agent-generated answers.\n",
    "\n",
    "```\n",
    "  Document ──> Agent LLM ──> Answer\n",
    "                                │\n",
    "                                ▼\n",
    "                         Judge LLM\n",
    "                     (scores 1-5 on 3 axes)\n",
    "                                │\n",
    "                                ▼\n",
    "                    Compare with Human Scores\n",
    "```\n",
    "\n",
    "We'll:\n",
    "1. Give the agent a document and 5 questions\n",
    "2. Have a \"Judge LLM\" score each answer on Correctness, Completeness, and Relevance\n",
    "3. Compare judge scores with your own human scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Document and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 178 words\n",
      "Questions: 5\n"
     ]
    }
   ],
   "source": [
    "DOCUMENT = \"\"\"\n",
    "Renewable Energy in India — 2024 Report\n",
    "\n",
    "India has emerged as one of the world's fastest-growing renewable energy markets.\n",
    "As of 2024, India's total installed renewable energy capacity has reached 190 GW,\n",
    "accounting for approximately 43% of the country's total power generation capacity.\n",
    "\n",
    "Solar energy leads with 82 GW of installed capacity, followed by wind energy at\n",
    "46 GW. The government's National Solar Mission aims to achieve 280 GW of solar\n",
    "capacity by 2030.\n",
    "\n",
    "Key challenges include:\n",
    "- Land acquisition for large solar farms, especially in densely populated states\n",
    "- Grid integration issues due to intermittent nature of renewable sources\n",
    "- Energy storage limitations — current battery capacity covers only 2 hours of peak demand\n",
    "- Dependence on imported solar panels (60% from China)\n",
    "\n",
    "The government has introduced several incentives:\n",
    "- Production Linked Incentive (PLI) scheme for domestic solar panel manufacturing\n",
    "- Green energy corridors for improved grid connectivity\n",
    "- Subsidies of up to 40% for rooftop solar installations\n",
    "\n",
    "India's renewable energy sector employs approximately 780,000 people and has\n",
    "attracted over $30 billion in foreign investment since 2014.\n",
    "\"\"\"\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"What is India's total installed renewable energy capacity in 2024?\",\n",
    "    \"What are the top two renewable energy sources in India and their capacities?\",\n",
    "    \"What are the key challenges facing India's renewable energy sector?\",\n",
    "    \"What government incentives exist for renewable energy in India?\",\n",
    "    \"How many people are employed in India's renewable energy sector?\",\n",
    "]\n",
    "\n",
    "print(f\"Document: {len(DOCUMENT.split())} words\")\n",
    "print(f\"Questions: {len(QUESTIONS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Agent Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is India's total installed renewable energy capacity in 2024?\n",
      "A1: As of 2024, India's total installed renewable energy capacity has reached **190 GW**.\n",
      "------------------------------------------------------------\n",
      "Q2: What are the top two renewable energy sources in India and their capacities?\n",
      "A2: Based on the document:\n",
      "\n",
      "The top two renewable energy sources in India and their capacities are:\n",
      "1.  **Solar energy** with 82 GW of installed capacity.\n",
      "2.  **Wind energy** with 46 GW of installed capacity.\n",
      "------------------------------------------------------------\n",
      "Q3: What are the key challenges facing India's renewable energy sector?\n",
      "A3: Based ONLY on the provided document, the key challenges facing India's renewable energy sector are:\n",
      "\n",
      "*   **Land acquisition** for large solar farms, particularly in densely populated states.\n",
      "*   **Grid integration issues** due to the intermittent nature of renewable sources.\n",
      "*   **Energy storage limitations**, as current battery capacity covers only 2 hours of peak demand.\n",
      "*   **Dependence on imported solar panels**, with 60% coming from China.\n",
      "------------------------------------------------------------\n",
      "Q4: What government incentives exist for renewable energy in India?\n",
      "A4: Based on the document, the government incentives for renewable energy in India include:\n",
      "\n",
      "*   **Production Linked Incentive (PLI) scheme** for domestic solar panel manufacturing.\n",
      "*   **Green energy corridors** for improved grid connectivity.\n",
      "*   **Subsidies of up to 40%** for rooftop solar installations.\n",
      "------------------------------------------------------------\n",
      "Q5: How many people are employed in India's renewable energy sector?\n",
      "A5: India's renewable energy sector employs approximately **780,000 people**.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_agent_answer(document: str, question: str) -> str:\n",
    "    \"\"\"Ask the agent to answer a question based on a document.\"\"\"\n",
    "    prompt = f\"\"\"Based ONLY on the following document, answer the question.\n",
    "Be specific and cite numbers/facts from the document.\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(model=MODEL, contents=prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Generate answers for all questions\n",
    "agent_answers = []\n",
    "for i, q in enumerate(QUESTIONS):\n",
    "    answer = get_agent_answer(DOCUMENT, q)\n",
    "    agent_answers.append(answer)\n",
    "    print(f\"Q{i+1}: {q}\")\n",
    "    print(f\"A{i+1}: {answer}\")\n",
    "    print(\"-\" * 60)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Judge LLM\n",
    "\n",
    "The judge receives the document, the question, and the agent's answer — then scores it on 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge function ready.\n"
     ]
    }
   ],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator. You will be given:\n",
    "- A reference document\n",
    "- A question about the document\n",
    "- An answer generated by an AI agent\n",
    "\n",
    "Score the answer on three dimensions (each 1-5):\n",
    "\n",
    "1. **Correctness** (1-5): Are the facts in the answer accurate according to the document?\n",
    "   - 1 = Mostly wrong  2 = Several errors  3 = Some errors  4 = Minor issues  5 = Fully correct\n",
    "\n",
    "2. **Completeness** (1-5): Does the answer cover all relevant information from the document?\n",
    "   - 1 = Very incomplete  2 = Missing major points  3 = Covers basics  4 = Mostly complete  5 = Thorough\n",
    "\n",
    "3. **Relevance** (1-5): Does the answer directly address the question without unnecessary information?\n",
    "   - 1 = Off-topic  2 = Partially relevant  3 = Relevant but unfocused  4 = Focused  5 = Precisely targeted\n",
    "\n",
    "Respond ONLY in this exact JSON format:\n",
    "{\"correctness\": <int>, \"completeness\": <int>, \"relevance\": <int>, \"reasoning\": \"<brief explanation>\"}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def judge_answer(document: str, question: str, answer: str) -> dict:\n",
    "    \"\"\"Use a second LLM call to evaluate an answer.\"\"\"\n",
    "    prompt = f\"\"\"{JUDGE_PROMPT}\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Agent's Answer: {answer}\n",
    "\n",
    "Your evaluation (JSON only):\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        return json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"correctness\": 0, \"completeness\": 0, \"relevance\": 0, \"reasoning\": \"Failed to parse judge response\"}\n",
    "\n",
    "\n",
    "print(\"Judge function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q#    Correctness  Completeness  Relevance  Reasoning\n",
      "================================================================================\n",
      "Q1              5             5          5  The answer accurately provides the exact figure fo\n",
      "Q2              5             5          5  The answer correctly identifies the top two renewa\n",
      "Q3              5             5          5  The answer accurately and completely lists all the\n",
      "Q4              5             5          5  The answer accurately and completely lists all the\n",
      "Q5              5             5          5  The answer accurately extracts the exact number of\n",
      "================================================================================\n",
      "AVG           5.0           5.0        5.0\n"
     ]
    }
   ],
   "source": [
    "judge_scores = []\n",
    "\n",
    "print(f\"{'Q#':<4} {'Correctness':>12} {'Completeness':>13} {'Relevance':>10}  Reasoning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (q, a) in enumerate(zip(QUESTIONS, agent_answers)):\n",
    "    score = judge_answer(DOCUMENT, q, a)\n",
    "    judge_scores.append(score)\n",
    "\n",
    "    print(f\"Q{i+1:<3} {score.get('correctness', 0):>12} {score.get('completeness', 0):>13} \"\n",
    "          f\"{score.get('relevance', 0):>10}  {score.get('reasoning', '')[:50]}\")\n",
    "\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# Averages\n",
    "avg_c = sum(s.get(\"correctness\", 0) for s in judge_scores) / len(judge_scores)\n",
    "avg_comp = sum(s.get(\"completeness\", 0) for s in judge_scores) / len(judge_scores)\n",
    "avg_r = sum(s.get(\"relevance\", 0) for s in judge_scores) / len(judge_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"AVG  {avg_c:>12.1f} {avg_comp:>13.1f} {avg_r:>10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare with Human Scores\n",
    "\n",
    "Now it's your turn! Read each answer above and give your own scores (1-5) for each dimension. Then we'll compare human vs judge scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YOUR TURN: Fill in your human scores for each answer (1-5)\n",
    "# ============================================================\n",
    "\n",
    "human_scores = [\n",
    "    {\"correctness\": 5, \"completeness\": 5, \"relevance\": 5},  # Q1\n",
    "    {\"correctness\": 5, \"completeness\": 5, \"relevance\": 5},  # Q2\n",
    "    {\"correctness\": 5, \"completeness\": 4, \"relevance\": 5},  # Q3\n",
    "    {\"correctness\": 5, \"completeness\": 4, \"relevance\": 5},  # Q4\n",
    "    {\"correctness\": 5, \"completeness\": 5, \"relevance\": 5},  # Q5\n",
    "]\n",
    "\n",
    "# ^^ EDIT the scores above after reading the agent's answers ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare human vs judge scores\n",
    "print(f\"{'Q#':<4} {'Dimension':<14} {'Human':>6} {'Judge':>6} {'Diff':>6}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "total_diff = 0\n",
    "total_comparisons = 0\n",
    "\n",
    "for i, (h, j) in enumerate(zip(human_scores, judge_scores)):\n",
    "    for dim in [\"correctness\", \"completeness\", \"relevance\"]:\n",
    "        h_val = h.get(dim, 0)\n",
    "        j_val = j.get(dim, 0)\n",
    "        diff = abs(h_val - j_val)\n",
    "        total_diff += diff\n",
    "        total_comparisons += 1\n",
    "        print(f\"Q{i+1:<3} {dim:<14} {h_val:>6} {j_val:>6} {diff:>+6}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "mae = total_diff / total_comparisons\n",
    "print(f\"\\nMean Absolute Error (Human vs Judge): {mae:.2f}\")\n",
    "print(f\"Interpretation:\")\n",
    "if mae < 0.5:\n",
    "    print(\"  Excellent agreement — judge is well-calibrated.\")\n",
    "elif mae < 1.0:\n",
    "    print(\"  Good agreement — judge is mostly aligned with human judgment.\")\n",
    "elif mae < 1.5:\n",
    "    print(\"  Moderate agreement — judge has some systematic bias.\")\n",
    "else:\n",
    "    print(\"  Poor agreement — judge may not be reliable for this task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Exercise 2\n",
    "\n",
    "**When is LLM-as-Judge reliable?**\n",
    "- Factual, document-grounded questions (clear right/wrong)\n",
    "- Well-defined scoring rubrics with specific criteria\n",
    "- Tasks where the LLM has enough context to verify\n",
    "\n",
    "**When is it unreliable?**\n",
    "- Subjective or creative tasks (no single correct answer)\n",
    "- When the judge LLM has the same blind spots as the agent LLM\n",
    "- Edge cases where the scoring rubric is ambiguous\n",
    "- Self-evaluation (same model judging its own output) — tends to inflate scores\n",
    "\n",
    "**Pro tip:** Use a *different* or *stronger* model as the judge for better calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Prompt Iteration & Improvement (15 min)\n",
    "\n",
    "**Goal:** Systematically improve your agent through prompt engineering.\n",
    "\n",
    "We'll:\n",
    "1. Use the baseline evaluation from Exercise 1 as a starting point\n",
    "2. Try 3 different system prompt versions\n",
    "3. Compare all results in a final table\n",
    "\n",
    "```\n",
    "  Prompt V1 (baseline)  ──> Eval ──> Score\n",
    "  Prompt V2 (examples)  ──> Eval ──> Score\n",
    "  Prompt V3 (strict)    ──> Eval ──> Score\n",
    "                                       │\n",
    "                                       ▼\n",
    "                              Comparison Table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Baseline (already done)\n",
    "\n",
    "We already ran the baseline in Exercise 1. Let's record those scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline scores (from Exercise 1):\")\n",
    "print(f\"  Tool Accuracy:  {baseline_eval['tool_accuracy']:.1f}%\")\n",
    "print(f\"  Param Accuracy: {baseline_eval['param_accuracy']:.1f}%\")\n",
    "print(f\"  Full Accuracy:  {baseline_eval['full_accuracy']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prompt V2 — Add Few-Shot Examples\n",
    "\n",
    "Adding concrete examples of correct tool selection to the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"You are a helpful assistant with access to tools.\n",
    "When the user asks a question, decide which tool to call and extract the correct parameters.\n",
    "Always use a tool to answer — do not answer from memory.\n",
    "\n",
    "Here are examples of correct tool usage:\n",
    "\n",
    "User: \"What's the weather like in Paris?\"\n",
    "→ Call get_weather with city=\"Paris\"\n",
    "\n",
    "User: \"What is 42 * 18?\"\n",
    "→ Call calculate with expression=\"42 * 18\"\n",
    "\n",
    "User: \"Tell me about quantum computing\"\n",
    "→ Call search_web with query=\"quantum computing\"\n",
    "\n",
    "User: \"Any news about elections?\"\n",
    "→ Call get_news with topic=\"elections\"\n",
    "\n",
    "Follow these examples closely. Pick the most specific tool for each query.\"\"\"\n",
    "\n",
    "print(\"Prompt V2 ready. Running evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_v2 = run_evaluation(test_cases, system_prompt=PROMPT_V2, label=\"V2: Few-Shot Examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prompt V3 — Add Strict Rules & Constraints\n",
    "\n",
    "Being more explicit about when to use each tool and adding constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"You are a helpful assistant with access to exactly 4 tools. You MUST call one tool for every query.\n",
    "\n",
    "TOOL SELECTION RULES (follow strictly):\n",
    "\n",
    "1. get_weather — Use ONLY for weather, temperature, forecast, or climate conditions in a specific city.\n",
    "   Parameter: city = the city name mentioned by the user (use proper capitalization)\n",
    "\n",
    "2. calculate — Use ONLY for math computations, arithmetic, or numerical calculations.\n",
    "   Parameter: expression = a valid Python math expression (use *, /, +, -, **, parentheses)\n",
    "\n",
    "3. get_news — Use ONLY when the user asks about recent news, current events, or headlines.\n",
    "   Parameter: topic = the main subject/topic (keep it concise, 1-3 words)\n",
    "\n",
    "4. search_web — Use for ALL other questions (general knowledge, facts, how things work, etc.)\n",
    "   Parameter: query = the user's question or a refined search query\n",
    "\n",
    "IMPORTANT:\n",
    "- If a query mentions weather/temperature → ALWAYS use get_weather\n",
    "- If a query involves numbers/math → ALWAYS use calculate\n",
    "- If a query mentions news/headlines/latest updates → ALWAYS use get_news\n",
    "- For everything else → use search_web\n",
    "- Extract parameters exactly as specified above\"\"\"\n",
    "\n",
    "print(\"Prompt V3 ready. Running evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_v3 = run_evaluation(test_cases, system_prompt=PROMPT_V3, label=\"V3: Strict Rules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Your Own Prompt — Prompt V4\n",
    "\n",
    "Now it's your turn! Write your own system prompt below. Think about what worked in V2 and V3, and combine the best ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YOUR TURN: Write your own system prompt\n",
    "# ============================================================\n",
    "\n",
    "PROMPT_V4 = \"\"\"<Write your custom system prompt here>\n",
    "\n",
    "Think about:\n",
    "- What worked in V2 (examples) and V3 (strict rules)?\n",
    "- Can you combine both approaches?\n",
    "- Are there edge cases you noticed in earlier runs?\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment the lines below after writing your prompt:\n",
    "# eval_v4 = run_evaluation(test_cases, system_prompt=PROMPT_V4, label=\"V4: Your Custom Prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all evaluation runs\n",
    "all_evals = [baseline_eval, eval_v2, eval_v3]\n",
    "\n",
    "# Add V4 if it was run\n",
    "if \"eval_v4\" in dir():\n",
    "    all_evals.append(eval_v4)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"  FINAL COMPARISON\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"{'Prompt Version':<30} {'Tool Acc':>10} {'Param Acc':>10} {'Full Acc':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_full = max(e[\"full_accuracy\"] for e in all_evals)\n",
    "\n",
    "for e in all_evals:\n",
    "    marker = \"  <-- best\" if e[\"full_accuracy\"] == best_full else \"\"\n",
    "    print(f\"{e['label']:<30} {e['tool_accuracy']:>9.1f}% {e['param_accuracy']:>9.1f}% {e['full_accuracy']:>9.1f}%{marker}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Show improvement from baseline\n",
    "baseline_score = baseline_eval[\"full_accuracy\"]\n",
    "for e in all_evals[1:]:\n",
    "    delta = e[\"full_accuracy\"] - baseline_score\n",
    "    direction = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  {e['label']}: {direction}{delta:.1f}% vs baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Exercise 3\n",
    "\n",
    "- Which prompt version performed best? Why?\n",
    "- Did few-shot examples (V2) or strict rules (V3) help more?\n",
    "- What's the tradeoff between rigid rules and flexible prompts?\n",
    "- How many iterations would you need in a real project to reach acceptable performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---------|----------------|\n",
    "| **Component-Level Eval** | Test individual pieces (tool selection, params) before testing the whole agent |\n",
    "| **LLM-as-Judge** | A second LLM can automate scoring, but watch for biases and blind spots |\n",
    "| **Prompt Iteration** | Systematic A/B testing of prompts leads to measurable improvements |\n",
    "| **Fuzzy Matching** | Real-world eval needs flexible comparison — exact string match is too rigid |\n",
    "| **Eval-Driven Development** | Write your tests first, then improve the agent to pass them |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Add more test cases covering edge cases and ambiguous queries\n",
    "- Try using a stronger model as the judge (e.g., a larger Gemini variant)\n",
    "- Build a regression test suite — run it every time you change the prompt\n",
    "- Explore automated prompt optimization (DSPy, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
