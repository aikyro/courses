{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Deep Dive: What LLMs Actually \"See\"\n",
    "\n",
    "This notebook explores **tokenization** — the process that converts raw text into the numerical units an LLM actually processes. Understanding tokenization is essential because it directly impacts:\n",
    "\n",
    "1. **Model Behavior** — Why LLMs fail at seemingly simple tasks (the \"Strawberry\" problem)\n",
    "2. **Multilingual Fairness** — Why non-English users pay more and get less\n",
    "3. **Cost & Limits** — How tokens translate to dollars and context window constraints\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer used by GPT-4 / GPT-4o\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def show_tokens(text, encoding=enc):\n",
    "    \"\"\"Visualize how a tokenizer splits text.\"\"\"\n",
    "    token_ids = encoding.encode(text)\n",
    "    tokens = [encoding.decode([tid]) for tid in token_ids]\n",
    "    print(f\"Text:       {repr(text)}\")\n",
    "    print(f\"Token IDs:  {token_ids}\")\n",
    "    print(f\"Tokens:     {tokens}\")\n",
    "    print(f\"Count:      {len(token_ids)} tokens\")\n",
    "    print()\n",
    "    return tokens, token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The \"Strawberry\" Problem\n",
    "\n",
    "A famous failure mode: ask an LLM *\"How many r's are in strawberry?\"* and it often answers **2** instead of **3**.\n",
    "\n",
    "This isn't a reasoning bug — it's a **tokenization** artifact. The model never sees individual letters. It sees *tokens*, which are subword chunks. Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HOW THE TOKENIZER SPLITS 'strawberry'\n",
      "============================================================\n",
      "Text:       'strawberry'\n",
      "Token IDs:  [302, 1618, 19772]\n",
      "Tokens:     ['st', 'raw', 'berry']\n",
      "Count:      3 tokens\n",
      "\n",
      "The model sees these chunks, NOT individual letters.\n",
      "To count 'r's, it would need to decompose tokens back\n",
      "into characters — something it was never trained to do.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HOW THE TOKENIZER SPLITS 'strawberry'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokens, ids = show_tokens(\"strawberry\")\n",
    "\n",
    "print(\"The model sees these chunks, NOT individual letters.\")\n",
    "print(\"To count 'r's, it would need to decompose tokens back\")\n",
    "print(\"into characters — something it was never trained to do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the problem more clearly\n",
    "\n",
    "Below we'll show that the letter boundaries *inside* a token are invisible to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHARACTER vs TOKEN VIEW\n",
      "============================================================\n",
      "\n",
      "What HUMANS see (character-level):\n",
      "   s | t | r | a | w | b | e | r | r | y\n",
      "  r count = 3  (easy — just scan each letter)\n",
      "\n",
      "What the MODEL sees (token-level):\n",
      "   'st' | 'raw' | 'berry'\n",
      "  The model must figure out how many 'r's hide INSIDE each token.\n",
      "  It has no direct letter-level access — only learned statistical patterns.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CHARACTER vs TOKEN VIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "word = \"strawberry\"\n",
    "token_ids = enc.encode(word)\n",
    "tokens = [enc.decode([tid]) for tid in token_ids]\n",
    "\n",
    "# Character-level view (what humans see)\n",
    "print(\"\\nWhat HUMANS see (character-level):\")\n",
    "print(\"  \", \" | \".join(list(word)))\n",
    "print(f\"  r count = {word.count('r')}  (easy — just scan each letter)\")\n",
    "\n",
    "# Token-level view (what the model sees)\n",
    "print(\"\\nWhat the MODEL sees (token-level):\")\n",
    "print(\"  \", \" | \".join([repr(t) for t in tokens]))\n",
    "print(f\"  The model must figure out how many 'r's hide INSIDE each token.\")\n",
    "print(f\"  It has no direct letter-level access — only learned statistical patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples where tokenization causes confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORDS THAT TRIP UP LLMs ON LETTER COUNTING\n",
      "============================================================\n",
      "\n",
      "'assessment' → tokens: ['assessment']\n",
      "  How many 's's? Actual = 4\n",
      "  The letter 's' is buried inside token boundaries — hard for the model.\n",
      "\n",
      "'mississippi' → tokens: ['miss', 'issippi']\n",
      "  How many 's's? Actual = 4\n",
      "  The letter 's' is buried inside token boundaries — hard for the model.\n",
      "\n",
      "'bookkeeper' → tokens: ['book', 'keeper']\n",
      "  How many 'o's? Actual = 2\n",
      "  The letter 'o' is buried inside token boundaries — hard for the model.\n",
      "\n",
      "'committee' → tokens: ['committee']\n",
      "  How many 'm's? Actual = 2\n",
      "  The letter 'm' is buried inside token boundaries — hard for the model.\n",
      "\n",
      "'occurrence' → tokens: ['occ', 'urrence']\n",
      "  How many 'c's? Actual = 3\n",
      "  The letter 'c' is buried inside token boundaries — hard for the model.\n"
     ]
    }
   ],
   "source": [
    "tricky_words = [\n",
    "    (\"assessment\", \"s\"),     # How many s's?\n",
    "    (\"mississippi\", \"s\"),    # How many s's?\n",
    "    (\"bookkeeper\", \"o\"),     # How many o's?\n",
    "    (\"committee\", \"m\"),      # How many m's?\n",
    "    (\"occurrence\", \"c\"),     # How many c's?\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORDS THAT TRIP UP LLMs ON LETTER COUNTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word, letter in tricky_words:\n",
    "    token_ids = enc.encode(word)\n",
    "    tokens = [enc.decode([tid]) for tid in token_ids]\n",
    "    actual_count = word.count(letter)\n",
    "    print(f\"\\n'{word}' → tokens: {tokens}\")\n",
    "    print(f\"  How many '{letter}'s? Actual = {actual_count}\")\n",
    "    print(f\"  The letter '{letter}' is buried inside token boundaries — hard for the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "> LLMs do not process text character-by-character. They process **tokens** — subword units learned from data via Byte Pair Encoding (BPE). Any task that requires character-level reasoning (counting letters, reversing strings, anagram solving) is fundamentally harder for LLMs because the characters are packed opaquely inside tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilingual Efficiency: Why Non-English Users Pay More\n",
    "\n",
    "Most tokenizers are trained primarily on English text. This means:\n",
    "- Common English words → **1 token** (efficient)\n",
    "- Non-English words → **many tokens** (inefficient)\n",
    "\n",
    "This has real consequences: more tokens = higher cost, slower inference, and faster context window exhaustion.\n",
    "\n",
    "Let's compare English against Hindi and other Indian languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENGLISH vs HINDI: SAME MEANING, DIFFERENT TOKEN COST\n",
      "======================================================================\n",
      "English                             Tokens  |  Hindi                               Tokens  |  Ratio\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Hello, how are you?                      6  |  नमस्ते, आप कैसे हैं?                     9  |  1.5x\n",
      "Artificial Intelligence                  2  |  कृत्रिम बुद्धिमत्ता                      7  |  3.5x\n",
      "machine learning                         2  |  मशीन लर्निंग                             6  |  3.0x\n",
      "The weather is nice today                5  |  आज मौसम अच्छा है                         4  |  0.8x\n",
      "What is your name?                       5  |  आपका नाम क्या है?                        6  |  1.2x\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ENGLISH vs HINDI: SAME MEANING, DIFFERENT TOKEN COST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pairs = [\n",
    "    (\"Hello, how are you?\", \"नमस्ते, आप कैसे हैं?\"),\n",
    "    (\"Artificial Intelligence\", \"कृत्रिम बुद्धिमत्ता\"),\n",
    "    (\"machine learning\", \"मशीन लर्निंग\"),\n",
    "    (\"The weather is nice today\", \"आज मौसम अच्छा है\"),\n",
    "    (\"What is your name?\", \"आपका नाम क्या है?\"),\n",
    "]\n",
    "\n",
    "print(f\"{'English':<35} {'Tokens':>6}  |  {'Hindi':<35} {'Tokens':>6}  |  Ratio\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for eng, hin in pairs:\n",
    "    eng_tokens = len(enc.encode(eng))\n",
    "    hin_tokens = len(enc.encode(hin))\n",
    "    ratio = hin_tokens / eng_tokens\n",
    "    print(f\"{eng:<35} {eng_tokens:>6}  |  {hin:<35} {hin_tokens:>6}  |  {ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see exactly how Hindi gets fragmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKEN-LEVEL BREAKDOWN: ENGLISH vs HINDI\n",
      "============================================================\n",
      "\n",
      "--- English ---\n",
      "Text:       'Artificial Intelligence'\n",
      "Token IDs:  [186671, 42378]\n",
      "Tokens:     ['Artificial', ' Intelligence']\n",
      "Count:      2 tokens\n",
      "\n",
      "--- Hindi ---\n",
      "Text:       'कृत्रिम बुद्धिमत्ता'\n",
      "Token IDs:  [1016, 10433, 11553, 15427, 155440, 15427, 75494]\n",
      "Tokens:     ['क', 'ृ', 'त्र', 'िम', ' बुद्ध', 'िम', 'त्ता']\n",
      "Count:      7 tokens\n",
      "\n",
      "Notice: English words map to 1-2 tokens each.\n",
      "Hindi characters get split into many byte-level fragments.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TOKEN-LEVEL BREAKDOWN: ENGLISH vs HINDI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- English ---\")\n",
    "show_tokens(\"Artificial Intelligence\")\n",
    "\n",
    "print(\"--- Hindi ---\")\n",
    "show_tokens(\"कृत्रिम बुद्धिमत्ता\")\n",
    "\n",
    "print(\"Notice: English words map to 1-2 tokens each.\")\n",
    "print(\"Hindi characters get split into many byte-level fragments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing across multiple Indian languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKEN COUNT: 'Artificial Intelligence' ACROSS LANGUAGES\n",
      "============================================================\n",
      "English        2 tokens  ██  (1.0x vs English)\n",
      "Hindi          7 tokens  ███████  (3.5x vs English)\n",
      "Tamil          9 tokens  █████████  (4.5x vs English)\n",
      "Telugu         9 tokens  █████████  (4.5x vs English)\n",
      "Kannada        8 tokens  ████████  (4.0x vs English)\n",
      "Malayalam      7 tokens  ███████  (3.5x vs English)\n",
      "Bengali        9 tokens  █████████  (4.5x vs English)\n",
      "Marathi        7 tokens  ███████  (3.5x vs English)\n",
      "\n",
      "English baseline: 2 tokens\n",
      "Every extra token = extra cost + latency + context consumed.\n"
     ]
    }
   ],
   "source": [
    "# \"Artificial Intelligence\" in multiple Indian languages\n",
    "translations = {\n",
    "    \"English\":    \"Artificial Intelligence\",\n",
    "    \"Hindi\":      \"कृत्रिम बुद्धिमत्ता\",\n",
    "    \"Tamil\":      \"செயற்கை நுண்ணறிவு\",\n",
    "    \"Telugu\":     \"కృత్రిమ మేధస్సు\",\n",
    "    \"Kannada\":    \"ಕೃತಕ ಬುದ್ಧಿಮತ್ತೆ\",\n",
    "    \"Malayalam\":  \"നിർമിത ബുദ്ധി\",\n",
    "    \"Bengali\":    \"কৃত্রিম বুদ্ধিমত্তা\",\n",
    "    \"Marathi\":    \"कृत्रिम बुद्धिमत्ता\",\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKEN COUNT: 'Artificial Intelligence' ACROSS LANGUAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eng_count = len(enc.encode(translations[\"English\"]))\n",
    "\n",
    "for lang, text in translations.items():\n",
    "    count = len(enc.encode(text))\n",
    "    bar = \"█\" * count\n",
    "    ratio = count / eng_count\n",
    "    print(f\"{lang:<12} {count:>3} tokens  {bar}  ({ratio:.1f}x vs English)\")\n",
    "\n",
    "print(f\"\\nEnglish baseline: {eng_count} tokens\")\n",
    "print(\"Every extra token = extra cost + latency + context consumed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does this happen?\n",
    "\n",
    "Tokenizers use **Byte Pair Encoding (BPE)** — they learn the most frequent character sequences from a training corpus. If the corpus is 90%+ English, then:\n",
    "- English subwords like `tion`, `ing`, `the` get dedicated tokens\n",
    "- Hindi characters like `क`, `ृ`, `त` remain as raw UTF-8 bytes, requiring multiple tokens per character\n",
    "\n",
    "This is not a bug — it's a **data distribution** consequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WHY ENGLISH IS EFFICIENT: COMMON SUBWORDS = SINGLE TOKENS\n",
      "============================================================\n",
      "  'the' → 1 token(s)\n",
      "  'tion' → 1 token(s)\n",
      "  'ing' → 1 token(s)\n",
      "  'ment' → 1 token(s)\n",
      "  'ness' → 1 token(s)\n",
      "  'able' → 1 token(s)\n",
      "\n",
      "These are so frequent in training data that BPE merges them early.\n",
      "Hindi/Tamil/Telugu characters never appear frequently enough to merge.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate: common English subwords get single tokens\n",
    "print(\"=\" * 60)\n",
    "print(\"WHY ENGLISH IS EFFICIENT: COMMON SUBWORDS = SINGLE TOKENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "english_subwords = [\"the\", \"tion\", \"ing\", \"ment\", \"ness\", \"able\"]\n",
    "for sw in english_subwords:\n",
    "    count = len(enc.encode(sw))\n",
    "    print(f\"  '{sw}' → {count} token(s)\")\n",
    "\n",
    "print(\"\\nThese are so frequent in training data that BPE merges them early.\")\n",
    "print(\"Hindi/Tamil/Telugu characters never appear frequently enough to merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "> A standard English-centric tokenizer can use **3-5x more tokens** for the same meaning in Indian languages. This directly translates to:\n",
    "> - **3-5x higher API costs** for the same content\n",
    "> - **3-5x faster context window exhaustion**\n",
    "> - **Slower inference** (more tokens to process)\n",
    ">\n",
    "> This is why multilingual models (like IndicBERT, MuRIL, or models with expanded vocabularies) exist — they add dedicated tokens for non-English scripts, dramatically reducing this overhead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost & Context Windows: The Business Side of Tokens\n",
    "\n",
    "Every API call to an LLM is priced **per token**. Understanding token economics is critical for building production applications.\n",
    "\n",
    "### Key concepts:\n",
    "- **Input tokens**: What you send to the model (prompt + context)\n",
    "- **Output tokens**: What the model generates (completion)\n",
    "- **Context window**: Maximum total tokens (input + output) the model can handle at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLM PRICING COMPARISON (per 1M tokens, USD)\n",
      "================================================================================\n",
      "Model                     Input   Output   Context Window\n",
      "--------------------------------------------------------------------------------\n",
      "GPT-4o                 $   2.50 $  10.00        128,000 tokens\n",
      "GPT-4o-mini            $   0.15 $   0.60        128,000 tokens\n",
      "Claude 3.5 Sonnet      $   3.00 $  15.00        200,000 tokens\n",
      "Claude 3.5 Haiku       $   0.80 $   4.00        200,000 tokens\n",
      "Gemini 2.0 Flash       $   0.10 $   0.40      1,000,000 tokens\n"
     ]
    }
   ],
   "source": [
    "# Current pricing (as of early 2025 — verify latest at provider sites)\n",
    "pricing = {\n",
    "    \"GPT-4o\": {\n",
    "        \"input\": 2.50,     # per 1M input tokens\n",
    "        \"output\": 10.00,   # per 1M output tokens\n",
    "        \"context\": 128_000,\n",
    "    },\n",
    "    \"GPT-4o-mini\": {\n",
    "        \"input\": 0.15,\n",
    "        \"output\": 0.60,\n",
    "        \"context\": 128_000,\n",
    "    },\n",
    "    \"Claude 3.5 Sonnet\": {\n",
    "        \"input\": 3.00,\n",
    "        \"output\": 15.00,\n",
    "        \"context\": 200_000,\n",
    "    },\n",
    "    \"Claude 3.5 Haiku\": {\n",
    "        \"input\": 0.80,\n",
    "        \"output\": 4.00,\n",
    "        \"context\": 200_000,\n",
    "    },\n",
    "    \"Gemini 2.0 Flash\": {\n",
    "        \"input\": 0.10,\n",
    "        \"output\": 0.40,\n",
    "        \"context\": 1_000_000,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LLM PRICING COMPARISON (per 1M tokens, USD)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<22} {'Input':>8} {'Output':>8} {'Context Window':>16}\")\n",
    "print(\"-\" * 80)\n",
    "for model, info in pricing.items():\n",
    "    print(f\"{model:<22} ${info['input']:>7.2f} ${info['output']:>7.2f} {info['context']:>14,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world cost calculator\n",
    "\n",
    "Let's calculate actual costs for common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCENARIO: Summarize a 5-page document (expect ~200 token summary)\n",
      "======================================================================\n",
      "\n",
      "GPT-4o:\n",
      "  Input:  3,001 tokens → $0.0075\n",
      "  Output: 200 tokens → $0.0020\n",
      "  Total:  $0.0095\n",
      "\n",
      "GPT-4o-mini:\n",
      "  Input:  3,001 tokens → $0.0005\n",
      "  Output: 200 tokens → $0.0001\n",
      "  Total:  $0.0006\n",
      "\n",
      "Claude 3.5 Sonnet:\n",
      "  Input:  3,001 tokens → $0.0090\n",
      "  Output: 200 tokens → $0.0030\n",
      "  Total:  $0.0120\n",
      "\n",
      "Claude 3.5 Haiku:\n",
      "  Input:  3,001 tokens → $0.0024\n",
      "  Output: 200 tokens → $0.0008\n",
      "  Total:  $0.0032\n",
      "\n",
      "Gemini 2.0 Flash:\n",
      "  Input:  3,001 tokens → $0.0003\n",
      "  Output: 200 tokens → $0.0001\n",
      "  Total:  $0.0004\n"
     ]
    }
   ],
   "source": [
    "def estimate_cost(input_text, output_tokens_est, model_name=\"GPT-4o\"):\n",
    "    \"\"\"Estimate API cost for a given input text and expected output length.\"\"\"\n",
    "    input_tokens = len(enc.encode(input_text))\n",
    "    model = pricing[model_name]\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * model[\"input\"]\n",
    "    output_cost = (output_tokens_est / 1_000_000) * model[\"output\"]\n",
    "    total = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens_est,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total,\n",
    "    }\n",
    "\n",
    "\n",
    "# Scenario: Summarize a 5-page document (~2500 words ≈ 3300 tokens)\n",
    "sample_doc = \"Lorem ipsum dolor sit amet. \" * 500  # ~3000 tokens\n",
    "output_est = 200  # summary\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCENARIO: Summarize a 5-page document (expect ~200 token summary)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in pricing:\n",
    "    result = estimate_cost(sample_doc, output_est, model_name)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Input:  {result['input_tokens']:,} tokens → ${result['input_cost']:.4f}\")\n",
    "    print(f\"  Output: {result['output_tokens']:,} tokens → ${result['output_cost']:.4f}\")\n",
    "    print(f\"  Total:  ${result['total_cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling costs: What happens at 10,000 requests/day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DAILY COST AT 10,000 REQUESTS/DAY\n",
      "(avg 1,000 input + 500 output tokens per request)\n",
      "======================================================================\n",
      "\n",
      "Model                       Daily      Monthly       Yearly\n",
      "------------------------------------------------------------\n",
      "GPT-4o                 $   75.00 $   2250.00 $  27375.00\n",
      "GPT-4o-mini            $    4.50 $    135.00 $   1642.50\n",
      "Claude 3.5 Sonnet      $  105.00 $   3150.00 $  38325.00\n",
      "Claude 3.5 Haiku       $   28.00 $    840.00 $  10220.00\n",
      "Gemini 2.0 Flash       $    3.00 $     90.00 $   1095.00\n",
      "\n",
      "Model choice at scale can mean the difference between $100/yr and $10,000+/yr.\n"
     ]
    }
   ],
   "source": [
    "daily_requests = 10_000\n",
    "avg_input_tokens = 1_000\n",
    "avg_output_tokens = 500\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"DAILY COST AT {daily_requests:,} REQUESTS/DAY\")\n",
    "print(f\"(avg {avg_input_tokens:,} input + {avg_output_tokens:,} output tokens per request)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Model':<22} {'Daily':>10} {'Monthly':>12} {'Yearly':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, info in pricing.items():\n",
    "    daily_input_cost = (daily_requests * avg_input_tokens / 1_000_000) * info[\"input\"]\n",
    "    daily_output_cost = (daily_requests * avg_output_tokens / 1_000_000) * info[\"output\"]\n",
    "    daily_total = daily_input_cost + daily_output_cost\n",
    "    monthly = daily_total * 30\n",
    "    yearly = daily_total * 365\n",
    "    print(f\"{model_name:<22} ${daily_total:>8.2f} ${monthly:>10.2f} ${yearly:>10.2f}\")\n",
    "\n",
    "print(\"\\nModel choice at scale can mean the difference between $100/yr and $10,000+/yr.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Window: What does 128K tokens actually look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTEXT WINDOW SIZE — WHAT FITS INSIDE?\n",
      "============================================================\n",
      "\n",
      "Context Size                           ≈ Words    ≈ Pages    ≈ Books\n",
      "----------------------------------------------------------------------\n",
      "4K tokens (GPT-3.5 original)             3,000         12        0.0\n",
      "8K tokens                                6,000         24        0.1\n",
      "32K tokens                              24,000         96        0.3\n",
      "128K tokens (GPT-4o)                    96,000        384        1.3\n",
      "200K tokens (Claude 3.5)               150,000        600        2.0\n",
      "1M tokens (Gemini 2.0)                 750,000       3000       10.0\n",
      "\n",
      "Context window = the model's SHORT-TERM MEMORY.\n",
      "Everything beyond this limit is invisible to the model.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CONTEXT WINDOW SIZE — WHAT FITS INSIDE?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Rough estimates: 1 token ≈ 0.75 English words, 1 page ≈ 250 words\n",
    "estimates = {\n",
    "    \"4K tokens (GPT-3.5 original)\": 4_000,\n",
    "    \"8K tokens\": 8_000,\n",
    "    \"32K tokens\": 32_000,\n",
    "    \"128K tokens (GPT-4o)\": 128_000,\n",
    "    \"200K tokens (Claude 3.5)\": 200_000,\n",
    "    \"1M tokens (Gemini 2.0)\": 1_000_000,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Context Size':<35} {'≈ Words':>10} {'≈ Pages':>10} {'≈ Books':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for label, tokens in estimates.items():\n",
    "    words = int(tokens * 0.75)\n",
    "    pages = words / 250\n",
    "    books = pages / 300  # avg novel ≈ 300 pages\n",
    "    print(f\"{label:<35} {words:>10,} {pages:>10.0f} {books:>10.1f}\")\n",
    "\n",
    "print(\"\\nContext window = the model's SHORT-TERM MEMORY.\")\n",
    "print(\"Everything beyond this limit is invisible to the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multilingual cost penalty in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTILINGUAL COST PENALTY: ENGLISH vs HINDI CUSTOMER SUPPORT BOT\n",
      "======================================================================\n",
      "\n",
      "English query: 50 tokens\n",
      "Hindi query:   76 tokens (same meaning!)\n",
      "Ratio:         1.5x\n",
      "\n",
      "At 100,000 queries/month (GPT-4o):\n",
      "  English: $312.50/month\n",
      "  Hindi:   $319.00/month\n",
      "  Extra cost for Hindi: $6.50/month\n",
      "\n",
      "Same product, same meaning — Hindi users cost more to serve.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MULTILINGUAL COST PENALTY: ENGLISH vs HINDI CUSTOMER SUPPORT BOT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulating a customer support scenario\n",
    "english_query = \"\"\"Hello, I purchased a laptop from your store last week and the \n",
    "screen has started flickering. I would like to request a replacement or refund. \n",
    "My order number is 12345. Please help me resolve this issue as soon as possible.\"\"\"\n",
    "\n",
    "hindi_query = \"\"\"नमस्ते, मैंने पिछले हफ्ते आपकी दुकान से एक लैपटॉप खरीदा था और \n",
    "स्क्रीन टिमटिमाने लगी है। मैं प्रतिस्थापन या धनवापसी का अनुरोध करना चाहता हूं। \n",
    "मेरा ऑर्डर नंबर 12345 है। कृपया इस समस्या को जल्द से जल्द हल करने में मेरी मदद करें।\"\"\"\n",
    "\n",
    "eng_tokens = len(enc.encode(english_query))\n",
    "hin_tokens = len(enc.encode(hindi_query))\n",
    "\n",
    "print(f\"\\nEnglish query: {eng_tokens} tokens\")\n",
    "print(f\"Hindi query:   {hin_tokens} tokens (same meaning!)\")\n",
    "print(f\"Ratio:         {hin_tokens/eng_tokens:.1f}x\")\n",
    "\n",
    "# Cost at 100K queries/month using GPT-4o\n",
    "monthly_queries = 100_000\n",
    "avg_output = 300  # tokens\n",
    "model = pricing[\"GPT-4o\"]\n",
    "\n",
    "eng_monthly = (monthly_queries * eng_tokens / 1_000_000) * model[\"input\"] + \\\n",
    "              (monthly_queries * avg_output / 1_000_000) * model[\"output\"]\n",
    "hin_monthly = (monthly_queries * hin_tokens / 1_000_000) * model[\"input\"] + \\\n",
    "              (monthly_queries * avg_output / 1_000_000) * model[\"output\"]\n",
    "\n",
    "print(f\"\\nAt {monthly_queries:,} queries/month (GPT-4o):\")\n",
    "print(f\"  English: ${eng_monthly:,.2f}/month\")\n",
    "print(f\"  Hindi:   ${hin_monthly:,.2f}/month\")\n",
    "print(f\"  Extra cost for Hindi: ${hin_monthly - eng_monthly:,.2f}/month\")\n",
    "print(f\"\\nSame product, same meaning — Hindi users cost more to serve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "> Tokens are the fundamental currency of LLMs. They determine:\n",
    "> - **What you pay** — input/output tokens are billed separately, output is more expensive\n",
    "> - **What the model can see** — the context window is a hard memory limit, not a soft suggestion\n",
    "> - **Who gets penalized** — non-English users consume more tokens for the same information, paying more and fitting less into context\n",
    ">\n",
    "> Smart token management (choosing the right model, compressing prompts, using cheaper models for simple tasks) is a core production engineering skill.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Build Your Intuition\n",
    "\n",
    "Use the interactive cell below to tokenize any text and see the breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "YOUR CUSTOM TOKENIZATION\n",
      "============================================================\n",
      "Text:       'Replace this with any text you want to tokenize'\n",
      "Token IDs:  [37050, 495, 483, 1062, 2201, 481, 1682, 316, 192720]\n",
      "Tokens:     ['Replace', ' this', ' with', ' any', ' text', ' you', ' want', ' to', ' tokenize']\n",
      "Count:      9 tokens\n",
      "\n",
      "Cost to process this as INPUT across models:\n",
      "  GPT-4o                 $0.000023\n",
      "  GPT-4o-mini            $0.000001\n",
      "  Claude 3.5 Sonnet      $0.000027\n",
      "  Claude 3.5 Haiku       $0.000007\n",
      "  Gemini 2.0 Flash       $0.000001\n"
     ]
    }
   ],
   "source": [
    "# Try your own text!\n",
    "your_text = \"Replace this with any text you want to tokenize\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"YOUR CUSTOM TOKENIZATION\")\n",
    "print(\"=\" * 60)\n",
    "tokens, ids = show_tokens(your_text)\n",
    "\n",
    "# Cost estimate across models\n",
    "n = len(ids)\n",
    "print(f\"Cost to process this as INPUT across models:\")\n",
    "for model_name, info in pricing.items():\n",
    "    cost = (n / 1_000_000) * info[\"input\"]\n",
    "    print(f\"  {model_name:<22} ${cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Why It Matters |\n",
    "|---|---|\n",
    "| **Strawberry Problem** | LLMs see tokens, not characters — letter-level tasks are inherently difficult |\n",
    "| **Multilingual Efficiency** | English-centric tokenizers penalize other languages with 3-5x token overhead |\n",
    "| **Cost & Context** | Tokens = money. Context window = memory. Both are hard limits you must design around |\n",
    "\n",
    "### Practical implications for engineers:\n",
    "- **Prompt compression** matters at scale — fewer tokens = lower cost\n",
    "- **Model selection** should factor in tokenizer efficiency for your language\n",
    "- **Context management** (chunking, summarization, RAG) is essential for long documents\n",
    "- **Character-level tasks** should be offloaded to code, not LLMs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
