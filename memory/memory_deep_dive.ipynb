{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory in LLM Applications — A Deep Dive\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this worksheet, you will:\n",
    "1. **Understand why memory matters** — see firsthand that LLM API calls are stateless\n",
    "2. **Implement short-term memory** — conversation history, sliding window, and summarization\n",
    "3. **Work with embeddings** — generate vectors, compute similarity, build a mini vector store\n",
    "4. **Build RAG from scratch** — chunk documents, embed, retrieve, and generate grounded answers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp 1.29.0 requires opentelemetry-exporter-otlp-proto-grpc==1.29.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp 1.29.0 requires opentelemetry-exporter-otlp-proto-http==1.29.0, but you have opentelemetry-exporter-otlp-proto-http 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet google-genai google-adk chromadb numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Google/Gemini API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "GOOGLE_API_KEY = getpass.getpass(\"Enter your Google/Gemini API key: \")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "print(\"API key configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory workshop ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_ID = \"gemini-2.0-flash\"\n",
    "EMBEDDING_MODEL = \"gemini-embedding-001\"\n",
    "\n",
    "# Quick test\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Say 'Memory workshop ready!' and nothing else.\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Problem — Why Memory Matters (5 min)\n",
    "\n",
    "Every call to an LLM API is **stateless**. The model has no memory of previous calls. Let's prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Demo: A Forgetful Chatbot\n",
    "\n",
    "We'll make 3 separate API calls and see if the model remembers anything between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Call 1 ---\n",
      "User: My name is Dr. Balamurali. I am the co-founder of AI Kyro.\n",
      "LLM:  Okay, Dr. Balamurali, it's nice to meet you. I understand you are the co-founder of AI Kyro. Is there anything specific you'd like to discuss or any information you'd like to share about AI Kyro? I'm ready to listen and assist in any way I can. For example, are you looking for help with:\n",
      "\n",
      "*   **Generating content for AI Kyro?** (e.g., website copy, social media posts, articles)\n",
      "*   **Exploring potential applications of AI Kyro's technology?**\n",
      "*   **Brainstorming ideas for marketing or outreach?**\n",
      "*   **Refining your elevator pitch?**\n",
      "*   **Something else entirely?**\n",
      "\n",
      "Just let me know!\n",
      "\n",
      "\n",
      "--- Call 2 ---\n",
      "User: What is my name?\n",
      "LLM:  I am an AI language model, and I do not know your name. You have not told me your name.\n",
      "\n",
      "\n",
      "--- Call 3 ---\n",
      "User: What do I teach?\n",
      "LLM:  To help me give you the best advice, tell me more about yourself! For example:\n",
      "\n",
      "*   **What is your educational background and expertise?** (e.g., degree in mathematics, experienced carpenter, certified yoga instructor)\n",
      "*   **What are your interests and passions?** What do you genuinely enjoy doing or talking about?\n",
      "*   **Who is your target audience?** (e.g., adults, children, beginners, advanced learners, people with specific needs)\n",
      "*   **Where do you plan to teach?** (e.g., online, in person, at a school, at a community center, privately)\n",
      "*   **What resources are available to you?** (e.g., equipment, space, funding)\n",
      "\n",
      "The more you tell me, the better I can help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stateless_call(user_message):\n",
    "    \"\"\"Each call is independent — no memory of previous calls.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=user_message\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Call 1: Introduce yourself\n",
    "print(\"--- Call 1 ---\")\n",
    "print(\"User: My name is Dr. Balamurali. I am the co-founder of AI Kyro.\")\n",
    "reply1 = stateless_call(\"My name is Dr. Balamurali. I am the co-founder of AI Kyro.\")\n",
    "print(f\"LLM:  {reply1}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Call 2: Ask the model your name\n",
    "print(\"--- Call 2 ---\")\n",
    "print(\"User: What is my name?\")\n",
    "reply2 = stateless_call(\"What is my name?\")\n",
    "print(f\"LLM:  {reply2}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Call 3: Ask about your profession\n",
    "print(\"--- Call 3 ---\")\n",
    "print(\"User: What do I teach?\")\n",
    "reply3 = stateless_call(\"What do I teach?\")\n",
    "print(f\"LLM:  {reply3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "The LLM has **no idea** who you are in Calls 2 and 3. Each API call starts from a blank slate.\n",
    "\n",
    "```\n",
    "Call 1: \"Hi, I'm Dr. Bala\"      → \"Nice to meet you!\"\n",
    "Call 2: \"What is my name?\"       → \"I don't know your name\"\n",
    "Call 3: \"What do I teach?\"       → \"I have no information about you\"\n",
    "```\n",
    "\n",
    "**This is the fundamental problem.** To build any useful chatbot or agent, we need to give it memory ourselves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Short-Term Memory — Conversation History (10 min)\n",
    "\n",
    "The simplest form of memory: keep a running list of all messages and send it every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Manual Chat History\n",
    "\n",
    "We maintain a `messages` list and append both user and assistant messages each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithHistory:\n",
    "    \"\"\"A chatbot that remembers by sending the full conversation each time.\"\"\"\n",
    "\n",
    "    def __init__(self, system_instruction=\"You are a helpful assistant.\"):\n",
    "        self.history = []  # This IS the memory\n",
    "        self.system_instruction = system_instruction\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        # Append the new user message to history\n",
    "        self.history.append(\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
    "        )\n",
    "\n",
    "        # Send the ENTIRE history to the model\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=self.history,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.text\n",
    "\n",
    "        # Append the assistant's reply to history\n",
    "        self.history.append(\n",
    "            types.Content(role=\"model\", parts=[types.Part(text=assistant_reply)])\n",
    "        )\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def show_history_size(self):\n",
    "        total_chars = sum(\n",
    "            len(part.text) for msg in self.history for part in msg.parts\n",
    "        )\n",
    "        print(f\"Messages in history: {len(self.history)}\")\n",
    "        print(f\"Total characters:    {total_chars:,}\")\n",
    "        print(f\"Estimated tokens:    ~{total_chars // 4:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Turn 1 ---\n",
      "User: My name is Dr. Bala. I teach AI courses.\n",
      "LLM:  Okay, Dr. Bala. It's a pleasure to meet you. As an AI assistant, I can definitely appreciate your work in teaching AI! How can I help you today? Perhaps you need help with:\n",
      "\n",
      "*   **Brainstorming lecture ideas?**\n",
      "*   **Finding resources for your students?**\n",
      "*   **Explaining a complex AI concept in a simple way?**\n",
      "*   **Developing assignments or projects?**\n",
      "*   **Or something else entirely?**\n",
      "\n",
      "Just let me know what you're working on.\n",
      "\n",
      "\n",
      "--- Turn 2 ---\n",
      "User: What is my name?\n",
      "LLM:  Your name is Dr. Bala, as you mentioned earlier.\n",
      "\n",
      "\n",
      "--- Turn 3 ---\n",
      "User: What do I teach?\n",
      "LLM:  You teach AI courses.\n",
      "\n",
      "\n",
      "Messages in history: 6\n",
      "Total characters:    578\n",
      "Estimated tokens:    ~144\n"
     ]
    }
   ],
   "source": [
    "bot = ChatWithHistory()\n",
    "\n",
    "# Same 3 messages — but now with history!\n",
    "print(\"--- Turn 1 ---\")\n",
    "print(\"User: My name is Dr. Bala. I teach AI courses.\")\n",
    "print(f\"LLM:  {bot.chat('My name is Dr. Bala. I teach AI courses.')}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Turn 2 ---\")\n",
    "print(\"User: What is my name?\")\n",
    "print(f\"LLM:  {bot.chat('What is my name?')}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Turn 3 ---\")\n",
    "print(\"User: What do I teach?\")\n",
    "print(f\"LLM:  {bot.chat('What do I teach?')}\")\n",
    "\n",
    "print()\n",
    "bot.show_history_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It remembers! But there's a catch...\n",
    "\n",
    "Every turn, we send **everything** back to the model. Let's see what happens with a long conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5 turns:\n",
      "Messages in history: 10\n",
      "Total characters:    739\n",
      "Estimated tokens:    ~184\n",
      "\n",
      "After 10 turns:\n",
      "Messages in history: 20\n",
      "Total characters:    1,411\n",
      "Estimated tokens:    ~352\n",
      "\n",
      "After 15 turns:\n",
      "Messages in history: 30\n",
      "Total characters:    2,075\n",
      "Estimated tokens:    ~518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate a long conversation to show context window filling up\n",
    "long_bot = ChatWithHistory(system_instruction=\"You are a travel advisor. Keep answers brief (1-2 sentences).\")\n",
    "\n",
    "travel_questions = [\n",
    "    \"I'm planning a trip to Japan in April.\",\n",
    "    \"What's the best city to see cherry blossoms?\",\n",
    "    \"How many days should I spend in Tokyo?\",\n",
    "    \"What about Kyoto? Is it worth visiting?\",\n",
    "    \"Should I get a Japan Rail Pass?\",\n",
    "    \"What's the food I must try?\",\n",
    "    \"Is it expensive compared to Europe?\",\n",
    "    \"Any tips for navigating the subway?\",\n",
    "    \"What about visiting Mount Fuji?\",\n",
    "    \"Should I learn some Japanese phrases?\",\n",
    "    \"What souvenirs should I bring back?\",\n",
    "    \"How's the weather in April?\",\n",
    "    \"Any day trips from Tokyo you recommend?\",\n",
    "    \"What about Osaka? Is it different from Tokyo?\",\n",
    "    \"Should I book hotels in advance?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(travel_questions, 1):\n",
    "    reply = long_bot.chat(question)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"After {i} turns:\")\n",
    "        long_bot.show_history_size()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the token count keeps growing. For Gemini Flash, the context window is ~1M tokens, but:\n",
    "- **Cost increases** with every turn (you pay per token sent)\n",
    "- **Latency increases** as the model processes more text\n",
    "- **Eventually**, even large context windows fill up\n",
    "\n",
    "We need smarter strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sliding Window Memory\n",
    "\n",
    "Keep only the last **N** messages. Old messages are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowChat:\n",
    "    \"\"\"Keeps only the last N message pairs in memory.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=6, system_instruction=\"You are a helpful assistant.\"):\n",
    "        self.full_history = []     # Everything (for comparison)\n",
    "        self.window_size = window_size  # Max messages to keep\n",
    "        self.system_instruction = system_instruction\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        # Append to full history (for tracking)\n",
    "        self.full_history.append(\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
    "        )\n",
    "\n",
    "        # Use only the sliding window\n",
    "        window = self.full_history[-self.window_size :]\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=window,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.text\n",
    "\n",
    "        self.full_history.append(\n",
    "            types.Content(role=\"model\", parts=[types.Part(text=assistant_reply)])\n",
    "        )\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def show_stats(self):\n",
    "        window = self.full_history[-self.window_size :]\n",
    "        window_chars = sum(len(p.text) for m in window for p in m.parts)\n",
    "        total_chars = sum(len(p.text) for m in self.full_history for p in m.parts)\n",
    "        print(f\"Total messages:    {len(self.full_history)}\")\n",
    "        print(f\"Window messages:   {len(window)}\")\n",
    "        print(f\"Window tokens:     ~{window_chars // 4:,}\")\n",
    "        print(f\"Saved tokens:      ~{(total_chars - window_chars) // 4:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Turn 1 ---\n",
      "LLM: Okay, Dr. Bala from Chennai. How can I help you today?\n",
      "\n",
      "\n",
      "(Added 4 filler turns)\n",
      "\n",
      "--- Turn 6 ---\n",
      "User: What is my name and where do I live?\n",
      "LLM:  As a large language model, I have no way of knowing your name or where you live. I have no memory of past conversations and no access to your personal information. You would need to tell me that information directly.\n",
      "\n",
      "\n",
      "Total messages:    12\n",
      "Window messages:   6\n",
      "Window tokens:     ~120\n",
      "Saved tokens:      ~93\n"
     ]
    }
   ],
   "source": [
    "# Demo: sliding window forgets early messages\n",
    "sw_bot = SlidingWindowChat(window_size=6)  # Only last 6 messages (3 turns)\n",
    "\n",
    "# Turn 1: Give it information\n",
    "print(\"--- Turn 1 ---\")\n",
    "print(f\"LLM: {sw_bot.chat('My name is Dr. Bala. I live in Chennai.')}\")\n",
    "\n",
    "# Turns 2-5: Fill the window with other topics\n",
    "filler = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me about photosynthesis in one sentence.\",\n",
    "    \"What year was Python created?\",\n",
    "    \"Name three programming paradigms.\",\n",
    "]\n",
    "for q in filler:\n",
    "    sw_bot.chat(q)\n",
    "print(f\"\\n(Added {len(filler)} filler turns)\\n\")\n",
    "\n",
    "# Turn 6: Can it still remember?\n",
    "print(\"--- Turn 6 ---\")\n",
    "print(\"User: What is my name and where do I live?\")\n",
    "print(f\"LLM:  {sw_bot.chat('What is my name and where do I live?')}\")\n",
    "\n",
    "print()\n",
    "sw_bot.show_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding window **forgets** your name because that message fell outside the window. Token usage stays constant, but important context is lost.\n",
    "\n",
    "Can we keep the best of both worlds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Summarization Strategy\n",
    "\n",
    "Use the LLM to **summarize** older messages into a compact paragraph. Keep:\n",
    "- A summary of everything old\n",
    "- The full recent messages\n",
    "\n",
    "This preserves key facts while saving tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizingChat:\n",
    "    \"\"\"Summarizes old messages to save tokens while preserving key facts.\"\"\"\n",
    "\n",
    "    def __init__(self, recent_window=6, system_instruction=\"You are a helpful assistant.\"):\n",
    "        self.history = []\n",
    "        self.summary = \"\"  # Running summary of older messages\n",
    "        self.recent_window = recent_window\n",
    "        self.system_instruction = system_instruction\n",
    "        self.total_messages_seen = 0\n",
    "\n",
    "    def _summarize_messages(self, messages):\n",
    "        \"\"\"Ask the LLM to summarize a block of messages.\"\"\"\n",
    "        conversation_text = \"\"\n",
    "        for msg in messages:\n",
    "            role = \"User\" if msg.role == \"user\" else \"Assistant\"\n",
    "            conversation_text += f\"{role}: {msg.parts[0].text}\\n\"\n",
    "\n",
    "        prompt = (\n",
    "            \"Summarize the following conversation into a concise paragraph. \"\n",
    "            \"Preserve all key facts (names, preferences, decisions, numbers). \"\n",
    "            \"Write it as a third-person summary.\\n\\n\"\n",
    "            f\"Previous summary: {self.summary}\\n\\n\"\n",
    "            f\"New conversation:\\n{conversation_text}\"\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "        return response.text\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        self.total_messages_seen += 1\n",
    "\n",
    "        self.history.append(\n",
    "            types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
    "        )\n",
    "\n",
    "        # If history is too long, summarize the older portion\n",
    "        if len(self.history) > self.recent_window * 2:\n",
    "            older_messages = self.history[: -self.recent_window]\n",
    "            self.summary = self._summarize_messages(older_messages)\n",
    "            self.history = self.history[-self.recent_window :]\n",
    "            print(f\"  [Summarized {len(older_messages)} older messages]\")\n",
    "\n",
    "        # Build the prompt: summary + recent messages\n",
    "        contents = []\n",
    "        if self.summary:\n",
    "            contents.append(\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part(text=f\"[Conversation summary so far: {self.summary}]\")]\n",
    "                )\n",
    "            )\n",
    "            contents.append(\n",
    "                types.Content(\n",
    "                    role=\"model\",\n",
    "                    parts=[types.Part(text=\"Understood, I'll keep this context in mind.\")]\n",
    "                )\n",
    "            )\n",
    "        contents.extend(self.history)\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=self.system_instruction\n",
    "            )\n",
    "        )\n",
    "\n",
    "        assistant_reply = response.text\n",
    "\n",
    "        self.history.append(\n",
    "            types.Content(role=\"model\", parts=[types.Part(text=assistant_reply)])\n",
    "        )\n",
    "\n",
    "        return assistant_reply\n",
    "\n",
    "    def show_stats(self):\n",
    "        history_chars = sum(len(p.text) for m in self.history for p in m.parts)\n",
    "        summary_chars = len(self.summary)\n",
    "        print(f\"Total turns seen:    {self.total_messages_seen}\")\n",
    "        print(f\"Messages in memory:  {len(self.history)}\")\n",
    "        print(f\"Summary length:      {summary_chars} chars (~{summary_chars // 4} tokens)\")\n",
    "        print(f\"Recent history:      {history_chars} chars (~{history_chars // 4} tokens)\")\n",
    "        print(f\"Total sent per call: ~{(history_chars + summary_chars) // 4} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: My name is Dr. Bala. I live in Chennai and teach A...\n",
      "  → Okay, Dr. Bala. It's a pleasure to meet you (virtually)! So you're Dr. Bala, an AI instructor based ...\n",
      "\n",
      "Turn 2: My favorite programming language is Python....\n",
      "  → That's great! Python is a fantastic choice, especially for AI. It's known for its readability, exten...\n",
      "\n",
      "Turn 3: I have 2 cats named Pixel and Byte....\n",
      "  → Pixel and Byte! Those are absolutely perfect names for a programmer's cats, especially an AI instruc...\n",
      "\n",
      "Turn 4: I prefer dark mode in all my editors....\n",
      "  → Ah, a person of culture! Dark mode is definitely the way to go. It's much easier on the eyes, especi...\n",
      "\n",
      "Turn 5: My birthday is March 15th....\n",
      "  → Okay, Dr. Bala! Noted. March 15th. I'll try my best to remember that. If you'd like, closer to the d...\n",
      "\n",
      "Turn 6: I'm currently researching multi-agent systems....\n",
      "  → Multi-agent systems are a fascinating and rapidly developing area of AI! That's a great topic to be ...\n",
      "\n",
      "  [Summarized 7 older messages]\n",
      "  [Summarized 8 older messages]\n",
      "Turn 12: Tell me about attention mechanisms....\n",
      "  → Okay, let's dive into attention mechanisms! They are a crucial component of modern deep learning, pa...\n",
      "\n",
      "============================================================\n",
      "Total turns seen:    12\n",
      "Messages in memory:  9\n",
      "Summary length:      440 chars (~110 tokens)\n",
      "Recent history:      6083 chars (~1520 tokens)\n",
      "Total sent per call: ~1630 tokens\n"
     ]
    }
   ],
   "source": [
    "# Demo: Summarizing chat preserves facts across many turns\n",
    "sum_bot = SummarizingChat(recent_window=6)\n",
    "\n",
    "# Give it lots of facts across many turns\n",
    "facts_and_questions = [\n",
    "    \"My name is Dr. Bala. I live in Chennai and teach AI courses.\",\n",
    "    \"My favorite programming language is Python.\",\n",
    "    \"I have 2 cats named Pixel and Byte.\",\n",
    "    \"I prefer dark mode in all my editors.\",\n",
    "    \"My birthday is March 15th.\",\n",
    "    \"I'm currently researching multi-agent systems.\",\n",
    "    \"What is the speed of light?\",\n",
    "    \"Explain gradient descent in one sentence.\",\n",
    "    \"What is a transformer architecture?\",\n",
    "    \"Name the planets in our solar system.\",\n",
    "    \"What is the difference between AI and ML?\",\n",
    "    \"Tell me about attention mechanisms.\",\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(facts_and_questions, 1):\n",
    "    reply = sum_bot.chat(msg)\n",
    "    if i <= 6 or i == len(facts_and_questions):\n",
    "        print(f\"Turn {i}: {msg[:50]}...\")\n",
    "        print(f\"  → {reply[:100]}...\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "sum_bot.show_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Memory After Summarization ---\n",
      "\n",
      "Q: What is my name?\n",
      "A: I don't have a name in the way humans do. I am a large language model, an AI assistant. You can call me assistant if you like!\n",
      "\n",
      "\n",
      "Q: What are my cats' names?\n",
      "A: You haven't told me your cats' names. However, I remember that Dr. Bala's cats are named Pixel and Byte. Since you are not Dr. Bala, I do not know your cats' names.\n",
      "\n",
      "\n",
      "Q: What am I researching?\n",
      "  [Summarized 8 older messages]\n",
      "A: You are currently researching multi-agent systems.\n",
      "\n",
      "\n",
      "\n",
      "--- Current Summary ---\n",
      "Dr. Bala, the AI instructor from Chennai who teaches AI courses and prefers dark mode with a birthday of March 15th, is researching multi-agent systems and knows the speed of light is approximately 299,792,458 meters per second. After requesting a one-sentence explanation of gradient descent, Dr. Bala, who still uses Python as their favorite programming language and owns two cats named Pixel and Byte, inquired about transformer architecture, the planets in our solar system, and the difference between AI and ML, as well as requesting an explanation of attention mechanisms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now test: does it still remember early facts?\n",
    "print(\"--- Testing Memory After Summarization ---\\n\")\n",
    "\n",
    "print(\"Q: What is my name?\")\n",
    "print(f\"A: {sum_bot.chat('What is my name?')}\\n\")\n",
    "\n",
    "print(\"Q: What are my cats' names?\")\n",
    "print(f\"A: {sum_bot.chat('What are my cats names?')}\\n\")\n",
    "\n",
    "print(\"Q: What am I researching?\")\n",
    "print(f\"A: {sum_bot.chat('What am I currently researching?')}\\n\")\n",
    "\n",
    "print(\"\\n--- Current Summary ---\")\n",
    "print(sum_bot.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Full History vs Summarized\n",
    "\n",
    "| Strategy | Tokens/call | Remembers early facts? | Cost at 100 turns |\n",
    "|----------|------------|----------------------|-------------------|\n",
    "| Full History | Grows linearly | Yes | High |\n",
    "| Sliding Window | Fixed (small) | No | Low |\n",
    "| Summarization | Fixed (medium) | Yes (compressed) | Medium |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Bonus: ADK Session Memory\n",
    "\n",
    "Google's Agent Development Kit (ADK) provides built-in session management. Let's see how it handles memory automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types as genai_types\n",
    "\n",
    "\n",
    "# Create a simple agent\n",
    "memory_agent = Agent(\n",
    "    name=\"memory_bot\",\n",
    "    model=MODEL_ID,\n",
    "    instruction=\"You are a friendly assistant. Remember everything the user tells you.\",\n",
    ")\n",
    "\n",
    "# ADK's session service manages conversation history automatically\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "runner = Runner(\n",
    "    agent=memory_agent,\n",
    "    app_name=\"memory_demo\",\n",
    "    session_service=session_service,\n",
    ")\n",
    "\n",
    "# Create a session — this is where memory lives\n",
    "session = await session_service.create_session(\n",
    "    app_name=\"memory_demo\",\n",
    "    user_id=\"dr_bala\"\n",
    ")\n",
    "\n",
    "print(f\"Session created: {session.id}\")\n",
    "print(f\"User ID: dr_bala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_agent(runner, session, user_message, user_id=\"dr_bala\"):\n",
    "    \"\"\"Send a message to the ADK agent and get a response.\"\"\"\n",
    "    user_content = genai_types.Content(\n",
    "        role=\"user\", parts=[genai_types.Part(text=user_message)]\n",
    "    )\n",
    "\n",
    "    final_response = \"\"\n",
    "    async for event in runner.run_async(\n",
    "        user_id=user_id,\n",
    "        session_id=session.id,\n",
    "        new_message=user_content,\n",
    "    ):\n",
    "        if event.content and event.content.parts:\n",
    "            for part in event.content.parts:\n",
    "                if part.text:\n",
    "                    final_response += part.text\n",
    "\n",
    "    return final_response\n",
    "\n",
    "\n",
    "# Test ADK memory across turns\n",
    "messages = [\n",
    "    \"My name is Dr. Bala. I teach AI at a university in Chennai.\",\n",
    "    \"My favorite framework is Google ADK.\",\n",
    "    \"What is my name and what do I teach?\",\n",
    "    \"What is my favorite framework?\",\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    reply = await chat_with_agent(runner, session, msg)\n",
    "    print(f\"User: {msg}\")\n",
    "    print(f\"Agent: {reply}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the session state — ADK stores the full conversation\n",
    "updated_session = await session_service.get_session(\n",
    "    app_name=\"memory_demo\",\n",
    "    user_id=\"dr_bala\",\n",
    "    session_id=session.id\n",
    ")\n",
    "\n",
    "print(f\"Events in session: {len(updated_session.events)}\")\n",
    "print(f\"\\nSession stores the full conversation history automatically.\")\n",
    "print(f\"ADK's InMemorySessionService = our ChatWithHistory, but built-in!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "ADK's `InMemorySessionService` handles short-term memory automatically. It stores conversation history in the session and sends it with every call — exactly what we built manually in section 2.1.\n",
    "\n",
    "But for **long-term memory** (remembering facts across sessions), we need something more powerful: **embeddings and vector stores**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Embeddings — The Foundation of Long-Term Memory (15 min)\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture **meaning**. Similar texts have similar vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 What Are Embeddings?\n",
    "\n",
    "Let's generate embeddings for a few sentences and look at what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'I love dogs'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [-0.0145, 0.0165, 0.0292, -0.0666, 0.0096]\n",
      "  ...it's just a list of 3072 numbers!\n",
      "\n",
      "'I adore puppies'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [-0.0257, -0.0105, 0.0354, -0.0529, -0.0145]\n",
      "  ...it's just a list of 3072 numbers!\n",
      "\n",
      "'The stock market crashed today'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [0.0105, 0.015, -0.0145, -0.0712, -0.0025]\n",
      "  ...it's just a list of 3072 numbers!\n",
      "\n",
      "'Machine learning is fascinating'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [-0.0132, 0.0286, 0.0133, -0.0755, -0.0168]\n",
      "  ...it's just a list of 3072 numbers!\n",
      "\n",
      "'Neural networks can learn patterns'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [-0.0069, 0.0201, 0.009, -0.0352, -0.0229]\n",
      "  ...it's just a list of 3072 numbers!\n",
      "\n",
      "'The weather is sunny and warm'\n",
      "  Vector length: 3072\n",
      "  First 5 values: [-0.0092, -0.003, -0.0123, -0.0604, -0.0209]\n",
      "  ...it's just a list of 3072 numbers!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get an embedding vector for a piece of text using Gemini.\"\"\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        contents=text\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "\n",
    "# Embed a few sentences\n",
    "sentences = [\n",
    "    \"I love dogs\",\n",
    "    \"I adore puppies\",\n",
    "    \"The stock market crashed today\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Neural networks can learn patterns\",\n",
    "    \"The weather is sunny and warm\",\n",
    "]\n",
    "\n",
    "embeddings = {}\n",
    "for sentence in sentences:\n",
    "    emb = get_embedding(sentence)\n",
    "    embeddings[sentence] = emb\n",
    "    print(f\"\\n'{sentence}'\")\n",
    "    print(f\"  Vector length: {len(emb)}\")\n",
    "    print(f\"  First 5 values: {[round(v, 4) for v in emb[:5]]}\")\n",
    "    print(f\"  ...it's just a list of {len(emb)} numbers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "Each sentence was converted into a **vector** — a list of 3072 floating-point numbers. These numbers encode the **meaning** of the text in a way that allows mathematical comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Similarity Search\n",
    "\n",
    "If embeddings capture meaning, then sentences with similar meanings should have similar vectors. Let's verify with **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix\n",
      "================================================================================\n",
      "                                  [0]  [1]  [2]  [3]  [4]  [5]\n",
      "[0]                  I love dogs 1.00 0.81 0.54 0.60 0.52 0.54\n",
      "[1]              I adore puppies 0.81 1.00 0.53 0.61 0.53 0.57\n",
      "[2] The stock market crashed today 0.54 0.53 1.00 0.54 0.53 0.55\n",
      "[3] Machine learning is fascinatin 0.60 0.61 0.54 1.00 0.66 0.58\n",
      "[4] Neural networks can learn patt 0.52 0.53 0.53 0.66 1.00 0.55\n",
      "[5] The weather is sunny and warm 0.54 0.57 0.55 0.58 0.55 1.00\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    a = np.array(vec_a)\n",
    "    b = np.array(vec_b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "# Compare all pairs\n",
    "print(\"Cosine Similarity Matrix\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Short labels for display\n",
    "labels = [s[:30] for s in sentences]\n",
    "\n",
    "# Print header\n",
    "print(f\"{'':>32}\", end=\"\")\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"  [{i}]\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, s1 in enumerate(sentences):\n",
    "    print(f\"[{i}] {labels[i]:>28}\", end=\"\")\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        sim = cosine_similarity(embeddings[s1], embeddings[s2])\n",
    "        print(f\" {sim:.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interesting Pairs:\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Similar meaning:\n",
      "    'I love dogs' vs 'I adore puppies'\n",
      "    Similarity: 0.8146  ████████████████████████\n",
      "\n",
      "  Different topics:\n",
      "    'I love dogs' vs 'The stock market crashed today'\n",
      "    Similarity: 0.5353  ████████████████\n",
      "\n",
      "  Related ML topics:\n",
      "    'Machine learning is fascinating' vs 'Neural networks can learn patterns'\n",
      "    Similarity: 0.6631  ███████████████████\n",
      "\n",
      "  Unrelated topics:\n",
      "    'The stock market crashed today' vs 'The weather is sunny and warm'\n",
      "    Similarity: 0.5484  ████████████████\n"
     ]
    }
   ],
   "source": [
    "# Highlight the interesting pairs\n",
    "print(\"\\nInteresting Pairs:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pairs = [\n",
    "    (\"I love dogs\", \"I adore puppies\", \"Similar meaning\"),\n",
    "    (\"I love dogs\", \"The stock market crashed today\", \"Different topics\"),\n",
    "    (\"Machine learning is fascinating\", \"Neural networks can learn patterns\", \"Related ML topics\"),\n",
    "    (\"The stock market crashed today\", \"The weather is sunny and warm\", \"Unrelated topics\"),\n",
    "]\n",
    "\n",
    "for s1, s2, label in pairs:\n",
    "    sim = cosine_similarity(embeddings[s1], embeddings[s2])\n",
    "    bar = \"█\" * int(sim * 30)\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    '{s1}' vs '{s2}'\")\n",
    "    print(f\"    Similarity: {sim:.4f}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "- \"I love dogs\" and \"I adore puppies\" have **high** similarity (~0.8+) even though they share no words!\n",
    "- \"I love dogs\" and \"Stock market crashed\" have **low** similarity — they're about completely different things\n",
    "\n",
    "Embeddings capture **semantic meaning**, not just word overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build a Mini Vector Store\n",
    "\n",
    "Now let's build a simple **vector store** — a searchable collection of facts stored as embeddings. This is RAG at its core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVectorStore:\n",
    "    \"\"\"A simple in-memory vector store. This is what ChromaDB/Pinecone/Weaviate do at scale.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.documents = []  # List of (text, embedding) tuples\n",
    "\n",
    "    def add(self, text):\n",
    "        \"\"\"Add a document to the store.\"\"\"\n",
    "        embedding = get_embedding(text)\n",
    "        self.documents.append((text, embedding))\n",
    "\n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Find the most similar documents to a query.\"\"\"\n",
    "        query_embedding = get_embedding(query)\n",
    "\n",
    "        # Compute similarity with every document\n",
    "        results = []\n",
    "        for text, doc_embedding in self.documents:\n",
    "            sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "            results.append((sim, text))\n",
    "\n",
    "        # Sort by similarity (highest first)\n",
    "        results.sort(reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded with 10 facts.\n"
     ]
    }
   ],
   "source": [
    "# Build a knowledge base about a user\n",
    "store = MiniVectorStore()\n",
    "\n",
    "user_facts = [\n",
    "    \"Dr. Bala is a professor who teaches AI and Machine Learning.\",\n",
    "    \"Dr. Bala lives in Chennai, India.\",\n",
    "    \"Dr. Bala's favorite language is Python.\",\n",
    "    \"Dr. Bala prefers dark mode in all applications.\",\n",
    "    \"Dr. Bala has two cats named Pixel and Byte.\",\n",
    "    \"Dr. Bala is researching multi-agent systems and autonomous AI.\",\n",
    "    \"Dr. Bala enjoys biryani and filter coffee.\",\n",
    "    \"Dr. Bala uses Google Colab and VS Code for development.\",\n",
    "    \"Dr. Bala's birthday is on March 15th.\",\n",
    "    \"Dr. Bala prefers concise explanations with code examples.\",\n",
    "]\n",
    "\n",
    "for fact in user_facts:\n",
    "    store.add(fact)\n",
    "\n",
    "print(f\"Vector store loaded with {len(store)} facts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'What does the user prefer?'\n",
      "  1. [0.6632] Dr. Bala prefers dark mode in all applications.\n",
      "  2. [0.6214] Dr. Bala prefers concise explanations with code examples.\n",
      "  3. [0.6026] Dr. Bala enjoys biryani and filter coffee.\n",
      "\n",
      "Query: 'Tell me about pets'\n",
      "  1. [0.6515] Dr. Bala has two cats named Pixel and Byte.\n",
      "  2. [0.5646] Dr. Bala's favorite language is Python.\n",
      "  3. [0.5630] Dr. Bala prefers concise explanations with code examples.\n",
      "\n",
      "Query: 'What food does he like?'\n",
      "  1. [0.6707] Dr. Bala enjoys biryani and filter coffee.\n",
      "  2. [0.6024] Dr. Bala's favorite language is Python.\n",
      "  3. [0.5760] Dr. Bala prefers concise explanations with code examples.\n",
      "\n",
      "Query: 'What is his job?'\n",
      "  1. [0.6070] Dr. Bala prefers concise explanations with code examples.\n",
      "  2. [0.5942] Dr. Bala's birthday is on March 15th.\n",
      "  3. [0.5932] Dr. Bala's favorite language is Python.\n",
      "\n",
      "Query: 'What tools does he use?'\n",
      "  1. [0.6315] Dr. Bala uses Google Colab and VS Code for development.\n",
      "  2. [0.6151] Dr. Bala prefers concise explanations with code examples.\n",
      "  3. [0.6019] Dr. Bala's favorite language is Python.\n"
     ]
    }
   ],
   "source": [
    "# Query the vector store\n",
    "queries = [\n",
    "    \"What does the user prefer?\",\n",
    "    \"Tell me about pets\",\n",
    "    \"What food does he like?\",\n",
    "    \"What is his job?\",\n",
    "    \"What tools does he use?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = store.search(query, top_k=3)\n",
    "    for rank, (score, text) in enumerate(results, 1):\n",
    "        print(f\"  {rank}. [{score:.4f}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is RAG at its core!\n",
    "\n",
    "What we just built:\n",
    "1. **Store** facts as embeddings\n",
    "2. **Query** with natural language\n",
    "3. **Retrieve** the most relevant facts\n",
    "\n",
    "The only step missing: pass the retrieved facts to an LLM to generate a grounded answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: RAG — Retrieval-Augmented Generation (15 min)\n",
    "\n",
    "RAG = **Retrieve** relevant context + **Augment** the prompt + **Generate** a grounded answer.\n",
    "\n",
    "Let's build it from scratch with a real document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build RAG from Scratch\n",
    "\n",
    "We'll use a sample course syllabus as our document, chunk it, embed it, store in ChromaDB, and query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 2185 characters, ~546 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a sample document (a course syllabus)\n",
    "SYLLABUS = \"\"\"\n",
    "CS 601: Advanced Artificial Intelligence — Spring 2025\n",
    "Instructor: Dr. Bala | Office: Room 302, CS Building | Office Hours: Mon/Wed 2-4 PM\n",
    "\n",
    "Course Description:\n",
    "This graduate-level course covers advanced topics in AI including deep learning architectures,\n",
    "natural language processing, computer vision, reinforcement learning, and multi-agent systems.\n",
    "Students will gain hands-on experience building AI systems using modern frameworks.\n",
    "\n",
    "Prerequisites:\n",
    "- CS 501: Introduction to Machine Learning (or equivalent)\n",
    "- Strong programming skills in Python\n",
    "- Linear algebra and probability theory\n",
    "\n",
    "Grading Policy:\n",
    "- Assignments (4 total): 40%\n",
    "- Midterm Exam: 20%\n",
    "- Final Project: 30%\n",
    "- Class Participation: 10%\n",
    "\n",
    "Assignment Policy:\n",
    "Late submissions receive a 10% penalty per day, up to 3 days. After 3 days, no submissions\n",
    "are accepted. One assignment may be dropped (the lowest score). Group work is allowed for\n",
    "assignments in teams of up to 3 students.\n",
    "\n",
    "Week-by-Week Schedule:\n",
    "Week 1-2: Review of ML fundamentals, gradient descent, backpropagation\n",
    "Week 3-4: Deep learning architectures — CNNs, RNNs, LSTMs\n",
    "Week 5-6: Transformer architecture and attention mechanisms\n",
    "Week 7: Midterm Exam\n",
    "Week 8-9: Large Language Models — GPT, BERT, fine-tuning\n",
    "Week 10-11: Prompt engineering and in-context learning\n",
    "Week 12-13: AI agents, tool use, and multi-agent systems\n",
    "Week 14-15: Final project presentations\n",
    "\n",
    "Required Textbooks:\n",
    "- \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
    "- \"Speech and Language Processing\" by Jurafsky and Martin (3rd edition, online)\n",
    "\n",
    "Final Project:\n",
    "Students must propose and implement an AI system that solves a real-world problem.\n",
    "Projects can be individual or in teams of up to 3. A 10-page report and a 15-minute\n",
    "presentation are required. The project proposal is due by Week 8.\n",
    "\n",
    "Academic Integrity:\n",
    "All work must be original. Use of AI tools (ChatGPT, Copilot) is allowed for learning\n",
    "but all AI-generated code must be clearly attributed. Plagiarism will result in a failing\n",
    "grade for the course.\n",
    "\n",
    "Tools and Platforms:\n",
    "- Python 3.10+\n",
    "- PyTorch or TensorFlow\n",
    "- Google Colab (free GPU access)\n",
    "- Hugging Face Transformers library\n",
    "- Weights & Biases for experiment tracking\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(SYLLABUS)} characters, ~{len(SYLLABUS) // 4} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 9 chunks:\n",
      "\n",
      "--- Chunk 1 (138 chars) ---\n",
      "CS 601: Advanced Artificial Intelligence — Spring 2025\n",
      "Instructor: Dr. Bala | Office: Room 302, CS Building | Office Hou...\n",
      "\n",
      "--- Chunk 2 (293 chars) ---\n",
      "Course Description:\n",
      "This graduate-level course covers advanced topics in AI including deep learning architectures,\n",
      "natur...\n",
      "\n",
      "--- Chunk 3 (264 chars) ---\n",
      "Prerequisites:\n",
      "- CS 501: Introduction to Machine Learning (or equivalent)\n",
      "- Strong programming skills in Python\n",
      "- Linear...\n",
      "\n",
      "--- Chunk 4 (241 chars) ---\n",
      "Assignment Policy:\n",
      "Late submissions receive a 10% penalty per day, up to 3 days. After 3 days, no submissions\n",
      "are accept...\n",
      "\n",
      "--- Chunk 5 (441 chars) ---\n",
      "Week-by-Week Schedule:\n",
      "Week 1-2: Review of ML fundamentals, gradient descent, backpropagation\n",
      "Week 3-4: Deep learning ar...\n",
      "\n",
      "--- Chunk 6 (154 chars) ---\n",
      "Required Textbooks:\n",
      "- \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
      "- \"Speech and Language Processing\" by Jurafsk...\n",
      "\n",
      "--- Chunk 7 (247 chars) ---\n",
      "Final Project:\n",
      "Students must propose and implement an AI system that solves a real-world problem.\n",
      "Projects can be indivi...\n",
      "\n",
      "--- Chunk 8 (217 chars) ---\n",
      "Academic Integrity:\n",
      "All work must be original. Use of AI tools (ChatGPT, Copilot) is allowed for learning\n",
      "but all AI-gen...\n",
      "\n",
      "--- Chunk 9 (171 chars) ---\n",
      "Tools and Platforms:\n",
      "- Python 3.10+\n",
      "- PyTorch or TensorFlow\n",
      "- Google Colab (free GPU access)\n",
      "- Hugging Face Transformers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Chunk the document into paragraphs\n",
    "\n",
    "def chunk_document(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks by paragraph boundaries.\"\"\"\n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) < chunk_size:\n",
    "            current_chunk += \"\\n\" + para if current_chunk else para\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = para\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = chunk_document(SYLLABUS)\n",
    "\n",
    "print(f\"Document split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i + 1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:120] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 9 chunks in ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Store chunks in ChromaDB\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()  # In-memory (ephemeral)\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"syllabus\",\n",
    "    metadata={\"description\": \"CS 601 course syllabus\"}\n",
    ")\n",
    "\n",
    "# Embed and store each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding = get_embedding(chunk)\n",
    "    collection.add(\n",
    "        ids=[f\"chunk_{i}\"],\n",
    "        embeddings=[embedding],\n",
    "        documents=[chunk],\n",
    "        metadatas=[{\"chunk_index\": i}]\n",
    "    )\n",
    "\n",
    "print(f\"Stored {collection.count()} chunks in ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the late submission policy?\n",
      "A: Late submissions receive a 10% penalty per day, up to 3 days. After 3 days, no submissions are accepted.\n",
      "\n",
      "   (Retrieved 3 chunks)\n",
      "\n",
      "Q: When is the midterm exam?\n",
      "A: Week 7\n",
      "\n",
      "   (Retrieved 3 chunks)\n",
      "\n",
      "Q: Can I use ChatGPT for assignments?\n",
      "A: Use of AI tools (ChatGPT, Copilot) is allowed for learning but all AI-generated code must be clearly attributed.\n",
      "\n",
      "   (Retrieved 3 chunks)\n",
      "\n",
      "Q: What textbooks are required?\n",
      "A: - \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
      "- \"Speech and Language Processing\" by Jurafsky and Martin (3rd edition, online)\n",
      "\n",
      "   (Retrieved 3 chunks)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build the RAG pipeline\n",
    "\n",
    "def rag_query(question, top_k=3):\n",
    "    \"\"\"Full RAG pipeline: embed query → retrieve chunks → generate answer.\"\"\"\n",
    "\n",
    "    # 1. Embed the question\n",
    "    query_embedding = get_embedding(question)\n",
    "\n",
    "    # 2. Retrieve top-k relevant chunks from ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    # 3. Build the augmented prompt\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Answer the following question based ONLY on the provided context. \"\n",
    "        f\"If the answer is not in the context, say 'This information is not in the syllabus.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 4. Generate the answer\n",
    "    response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.text,\n",
    "        \"sources\": retrieved_chunks,\n",
    "        \"distances\": distances\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the RAG pipeline\n",
    "questions = [\n",
    "    \"What is the late submission policy?\",\n",
    "    \"When is the midterm exam?\",\n",
    "    \"Can I use ChatGPT for assignments?\",\n",
    "    \"What textbooks are required?\",\n",
    "    \"How much is the final project worth?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_query(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   (Retrieved {len(result['sources'])} chunks)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 With vs Without RAG\n",
    "\n",
    "Let's compare answers to the same questions — one with RAG (grounded in the document) and one without (pure LLM generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_without_rag(question):\n",
    "    \"\"\"Compare RAG-augmented answers vs raw LLM answers.\"\"\"\n",
    "\n",
    "    # WITHOUT RAG — raw LLM call\n",
    "    raw_prompt = (\n",
    "        f\"Answer this question about the CS 601 Advanced AI course taught by Dr. Bala:\\n\\n\"\n",
    "        f\"{question}\"\n",
    "    )\n",
    "    raw_response = client.models.generate_content(model=MODEL_ID, contents=raw_prompt)\n",
    "    raw_answer = raw_response.text\n",
    "\n",
    "    # WITH RAG — grounded in the actual document\n",
    "    rag_result = rag_query(question)\n",
    "    rag_answer = rag_result[\"answer\"]\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"WITHOUT RAG (LLM guesses):\")\n",
    "    print(f\"  {raw_answer[:300]}\")\n",
    "    print()\n",
    "    print(f\"WITH RAG (grounded in document):\")\n",
    "    print(f\"  {rag_answer[:300]}\")\n",
    "    print(f\"{'=' * 70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on specific factual questions\n",
    "comparison_questions = [\n",
    "    \"What percentage of the grade comes from assignments?\",\n",
    "    \"What is the late penalty per day?\",\n",
    "    \"When is the project proposal due?\",\n",
    "    \"What topics are covered in weeks 8-9?\",\n",
    "]\n",
    "\n",
    "for q in comparison_questions:\n",
    "    compare_with_without_rag(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "| Aspect | Without RAG | With RAG |\n",
    "|--------|------------|----------|\n",
    "| **Accuracy** | Guesses/hallucinates specific numbers | Exact numbers from the document |\n",
    "| **Reliability** | May sound confident but be wrong | Grounded in actual source material |\n",
    "| **Traceability** | No source attribution | Can show which chunks were used |\n",
    "| **Cost** | Fewer tokens (no context) | More tokens (context included) |\n",
    "\n",
    "RAG trades a small amount of token cost for **dramatically better accuracy** on factual questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Bonus: RAG with ADK Agent\n",
    "\n",
    "Let's combine ADK's agent framework with our RAG pipeline. The agent gets a tool to search the syllabus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "\n",
    "def search_syllabus(query: str) -> dict:\n",
    "    \"\"\"Search the CS 601 course syllabus for relevant information.\n",
    "\n",
    "    Use this tool when the user asks about course policies, schedule,\n",
    "    grading, assignments, exams, or any course-related information.\n",
    "\n",
    "    Args:\n",
    "        query: The search query describing what information to find.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing relevant passages from the syllabus.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    return {\n",
    "        \"relevant_passages\": results[\"documents\"][0],\n",
    "        \"num_results\": len(results[\"documents\"][0])\n",
    "    }\n",
    "\n",
    "\n",
    "# Create the RAG-powered agent\n",
    "rag_agent = Agent(\n",
    "    name=\"syllabus_assistant\",\n",
    "    model=MODEL_ID,\n",
    "    instruction=(\n",
    "        \"You are a helpful course assistant for CS 601: Advanced AI. \"\n",
    "        \"When students ask about the course, ALWAYS use the search_syllabus tool \"\n",
    "        \"to find accurate information before answering. \"\n",
    "        \"Base your answers only on the retrieved information. \"\n",
    "        \"If the information isn't in the syllabus, say so.\"\n",
    "    ),\n",
    "    tools=[search_syllabus],\n",
    ")\n",
    "\n",
    "# Set up runner and session\n",
    "rag_session_service = InMemorySessionService()\n",
    "rag_runner = Runner(\n",
    "    agent=rag_agent,\n",
    "    app_name=\"rag_demo\",\n",
    "    session_service=rag_session_service,\n",
    ")\n",
    "\n",
    "rag_session = await rag_session_service.create_session(\n",
    "    app_name=\"rag_demo\",\n",
    "    user_id=\"student_1\"\n",
    ")\n",
    "\n",
    "print(\"RAG Agent ready! It has conversation memory (ADK) + document memory (ChromaDB).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with the RAG agent\n",
    "student_questions = [\n",
    "    \"Hi, I'm a new student. What are the prerequisites for this course?\",\n",
    "    \"What's the grading breakdown?\",\n",
    "    \"I submitted my assignment 2 days late. How much will I lose?\",\n",
    "    \"Can I work with a friend on the final project?\",\n",
    "]\n",
    "\n",
    "for question in student_questions:\n",
    "    reply = await chat_with_agent(rag_runner, rag_session, question, user_id=\"student_1\")\n",
    "    print(f\"Student: {question}\")\n",
    "    print(f\"Agent:   {reply}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: The Memory Stack\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│           Long-Term Memory (Part 3-4)           │\n",
    "│   Embeddings + Vector Store + RAG               │\n",
    "│   → Remembers facts across sessions             │\n",
    "│   → Retrieves relevant knowledge on demand      │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│         Short-Term Memory (Part 2)              │\n",
    "│   Conversation History / Sliding Window /       │\n",
    "│   Summarization / ADK Sessions                  │\n",
    "│   → Remembers within a session                  │\n",
    "│   → Manages token budget                        │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│           No Memory (Part 1)                    │\n",
    "│   Raw API Call                                  │\n",
    "│   → Stateless, forgets everything               │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## What We Built Today\n",
    "\n",
    "| Technique | Type | Remembers | Token Cost | Implementation |\n",
    "|-----------|------|-----------|------------|----------------|\n",
    "| Raw API call | None | Nothing | Minimal | `client.models.generate_content()` |\n",
    "| Chat history | Short-term | Full session | Grows linearly | `messages` list |\n",
    "| Sliding window | Short-term | Last N turns | Fixed | `messages[-N:]` |\n",
    "| Summarization | Short-term | Compressed history | Fixed (medium) | LLM summarizes old messages |\n",
    "| ADK sessions | Short-term | Full session | Grows linearly | `InMemorySessionService` |\n",
    "| Embeddings | Long-term | Semantic facts | Per-query | `embed_content()` + cosine similarity |\n",
    "| Mini vector store | Long-term | Stored facts | Per-query | Python dict + embeddings |\n",
    "| RAG (ChromaDB) | Long-term | Documents | Per-query + context | Chunk → Embed → Retrieve → Generate |\n",
    "| RAG + ADK agent | Both | Session + Documents | Combined | Agent with search tool |\n",
    "\n",
    "## Next Steps\n",
    "- **Persistent storage**: Use `DatabaseSessionService` (ADK) or persistent ChromaDB for data that survives restarts\n",
    "- **Hybrid memory**: Combine conversation history + user fact store + document RAG\n",
    "- **Memory management**: Implement importance scoring — not all facts are worth remembering\n",
    "- **Evaluation**: Measure retrieval quality with precision/recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "chroma_client.delete_collection(\"syllabus\")\n",
    "print(\"Workshop complete! ChromaDB collection cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
