{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution: Giving LLMs the Power to Run Code\n",
    "\n",
    "**Duration**: ~12 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand **why** LLMs need code execution (tool limitations)\n",
    "2. Build a simple **code execution pipeline** using `<execute_python>` tags\n",
    "3. See code execution handle math, analysis, and visualization tasks\n",
    "4. Learn safety considerations for running LLM-generated code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = input(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Problem: A Limited Calculator Agent\n",
    "\n",
    "Imagine you build an agent with only 4 math tools: **add**, **subtract**, **multiply**, and **divide**.\n",
    "\n",
    "What happens when a user asks for something outside those 4 operations — like a square root?\n",
    "\n",
    "Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Define 4 basic calculator tools\n",
    "calculator_tools = [\n",
    "    types.Tool(function_declarations=[\n",
    "        types.FunctionDeclaration(\n",
    "            name=\"add\",\n",
    "            description=\"Add two numbers together\",\n",
    "            parameters=types.Schema(\n",
    "                type=\"OBJECT\",\n",
    "                properties={\n",
    "                    \"a\": types.Schema(type=\"NUMBER\", description=\"First number\"),\n",
    "                    \"b\": types.Schema(type=\"NUMBER\", description=\"Second number\"),\n",
    "                },\n",
    "                required=[\"a\", \"b\"],\n",
    "            ),\n",
    "        ),\n",
    "        types.FunctionDeclaration(\n",
    "            name=\"subtract\",\n",
    "            description=\"Subtract second number from first\",\n",
    "            parameters=types.Schema(\n",
    "                type=\"OBJECT\",\n",
    "                properties={\n",
    "                    \"a\": types.Schema(type=\"NUMBER\", description=\"First number\"),\n",
    "                    \"b\": types.Schema(type=\"NUMBER\", description=\"Second number\"),\n",
    "                },\n",
    "                required=[\"a\", \"b\"],\n",
    "            ),\n",
    "        ),\n",
    "        types.FunctionDeclaration(\n",
    "            name=\"multiply\",\n",
    "            description=\"Multiply two numbers\",\n",
    "            parameters=types.Schema(\n",
    "                type=\"OBJECT\",\n",
    "                properties={\n",
    "                    \"a\": types.Schema(type=\"NUMBER\", description=\"First number\"),\n",
    "                    \"b\": types.Schema(type=\"NUMBER\", description=\"Second number\"),\n",
    "                },\n",
    "                required=[\"a\", \"b\"],\n",
    "            ),\n",
    "        ),\n",
    "        types.FunctionDeclaration(\n",
    "            name=\"divide\",\n",
    "            description=\"Divide first number by second\",\n",
    "            parameters=types.Schema(\n",
    "                type=\"OBJECT\",\n",
    "                properties={\n",
    "                    \"a\": types.Schema(type=\"NUMBER\", description=\"First number\"),\n",
    "                    \"b\": types.Schema(type=\"NUMBER\", description=\"Second number\"),\n",
    "                },\n",
    "                required=[\"a\", \"b\"],\n",
    "            ),\n",
    "        )\n",
    "    ])\n",
    "]\n",
    "\n",
    "# Force the model to use one of these tools (mode=ANY)\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config=types.FunctionCallingConfig(mode=\"ANY\")\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the square root of 2?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=calculator_tools,\n",
    "        tool_config=tool_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Show what the model tried to do\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.function_call:\n",
    "        print(f\"Tool called: {part.function_call.name}\")\n",
    "        print(f\"Arguments:   {dict(part.function_call.args)}\")\n",
    "        print()\n",
    "        print(\"The model was FORCED to pick from add/subtract/multiply/divide.\")\n",
    "        print(\"It has no sqrt tool, so it had to improvise (incorrectly).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Solution: `<execute_python>` Tags\n",
    "\n",
    "Instead of giving the LLM a fixed set of tools, we can give it **one universal tool: code execution**.\n",
    "\n",
    "The idea:\n",
    "1. Tell the LLM to write Python code inside `<execute_python>` tags\n",
    "2. Parse the code from its response\n",
    "3. Execute it and return the output\n",
    "\n",
    "This way, the LLM can solve **any** computable problem — not just the ones we pre-built tools for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "CODE_SYSTEM_PROMPT = \"\"\"You are a Python code assistant. When asked to compute or analyze something, \n",
    "write Python code inside <execute_python> tags. The code should use print() to output results.\n",
    "\n",
    "Example:\n",
    "User: What is 2 + 2?\n",
    "Assistant: <execute_python>\n",
    "print(2 + 2)\n",
    "</execute_python>\n",
    "\n",
    "Always write complete, runnable Python code. You have access to the math and statistics modules.\"\"\"\n",
    "\n",
    "\n",
    "def execute_code_from_response(response_text):\n",
    "    \"\"\"Extract and execute Python code from <execute_python> tags.\"\"\"\n",
    "    # Find all code blocks between tags\n",
    "    code_blocks = re.findall(\n",
    "        r\"<execute_python>(.*?)</execute_python>\", response_text, re.DOTALL\n",
    "    )\n",
    "\n",
    "    if not code_blocks:\n",
    "        return \"No code found in response.\"\n",
    "\n",
    "    results = []\n",
    "    for code in code_blocks:\n",
    "        code = code.strip()\n",
    "        # Capture stdout\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = captured = io.StringIO()\n",
    "\n",
    "        try:\n",
    "            # Execute with limited globals for safety\n",
    "            allowed_globals = {\n",
    "                \"__builtins__\": __builtins__,\n",
    "                \"math\": math,\n",
    "                \"statistics\": statistics,\n",
    "            }\n",
    "            exec(code, allowed_globals)\n",
    "            output = captured.getvalue()\n",
    "            results.append(output.strip())\n",
    "        except Exception as e:\n",
    "            results.append(f\"Error: {e}\")\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "def ask_with_code_execution(question):\n",
    "    \"\"\"Send a question to the LLM and execute any code in its response.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=question,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=CODE_SYSTEM_PROMPT,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    llm_text = response.text\n",
    "    print(\"LLM Response:\")\n",
    "    print(llm_text)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "    result = execute_code_from_response(llm_text)\n",
    "    print(f\"Execution Output: {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Code execution pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Factorial of 20\n",
    "print(\"DEMO 1: What is the factorial of 20?\")\n",
    "print(\"=\" * 50)\n",
    "ask_with_code_execution(\"What is the factorial of 20?\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Square root of 2 — the problem from earlier, now solved!\n",
    "print(\"DEMO 2: What is the square root of 2?\")\n",
    "print(\"=\" * 50)\n",
    "ask_with_code_execution(\"What is the square root of 2?\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Generate sin(x) values\n",
    "print(\"DEMO 3: Plot sin(x) from 0 to 2*pi\")\n",
    "print(\"=\" * 50)\n",
    "ask_with_code_execution(\n",
    "    \"Print a table of sin(x) values from 0 to 2*pi in 8 equal steps. \"\n",
    "    \"Format each line as: x_value -> sin(x_value), rounded to 4 decimals.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Exercise: Data Analysis via Code Execution\n",
    "\n",
    "**Your turn!** Modify the system prompt so the LLM performs basic data analysis.\n",
    "\n",
    "Try asking it to analyze a list of numbers — compute the mean, median, standard deviation, min, and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: ask the code execution pipeline to analyze this data\n",
    "# Hint: just call ask_with_code_execution() with a clear question!\n",
    "\n",
    "data = [23, 45, 12, 67, 34, 89, 56, 11, 78, 43]\n",
    "\n",
    "# TODO: call ask_with_code_execution with a question about `data`\n",
    "# Example: ask_with_code_execution(f\"Analyze this list: {data}. Compute mean, median, stdev, min, max.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Advanced: Code Execution with Package Installation\n",
    "\n",
    "In the basic pipeline above, we restricted the LLM to only `math` and `statistics`. But real-world tasks often need **external packages** — pandas for data analysis, matplotlib for visualization, numpy for numerical computing, requests for web APIs, etc.\n",
    "\n",
    "This is exactly what tools like **ChatGPT Code Interpreter** do: they let the LLM install and use any package.\n",
    "\n",
    "**What we'll build next:**\n",
    "- An upgraded executor that detects `import` statements in LLM-generated code\n",
    "- Auto-installs missing packages via `pip install`\n",
    "- Runs code with full `__builtins__` (no restricted globals)\n",
    "- Handles matplotlib plots by saving to file and displaying inline\n",
    "\n",
    "**Safety note:** This is powerful but dangerous in production. Always use containers/sandboxes for untrusted code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "ADVANCED_SYSTEM_PROMPT = \"\"\"You are a Python code assistant with access to ANY Python package.\n",
    "When asked to compute, analyze, or visualize something, write Python code inside <execute_python> tags.\n",
    "Use print() to output results. For plots, use matplotlib and call plt.savefig('_output.png') then plt.close().\n",
    "You can freely use: pandas, numpy, matplotlib, requests, or any other package.\n",
    "\n",
    "Example:\n",
    "User: Analyze this data\n",
    "Assistant: <execute_python>\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'values': [1, 2, 3]})\n",
    "print(df.describe())\n",
    "</execute_python>\n",
    "\n",
    "Always write complete, runnable Python code with print() for all outputs.\"\"\"\n",
    "\n",
    "\n",
    "def auto_install_packages(code):\n",
    "    \"\"\"Detect import statements and auto-install missing packages.\"\"\"\n",
    "    # Find all top-level package names from import statements\n",
    "    imports = re.findall(r'^\\s*(?:import|from)\\s+(\\w+)', code, re.MULTILINE)\n",
    "    # Map common import names to pip package names\n",
    "    pip_name_map = {\n",
    "        \"cv2\": \"opencv-python\",\n",
    "        \"sklearn\": \"scikit-learn\",\n",
    "        \"PIL\": \"Pillow\",\n",
    "        \"bs4\": \"beautifulsoup4\",\n",
    "    }\n",
    "    for pkg in set(imports):\n",
    "        try:\n",
    "            __import__(pkg)\n",
    "        except ImportError:\n",
    "            pip_name = pip_name_map.get(pkg, pkg)\n",
    "            print(f\"Installing {pip_name}...\")\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            print(f\"Installed {pip_name} successfully.\")\n",
    "\n",
    "\n",
    "def advanced_execute_code(code):\n",
    "    \"\"\"Execute code with full builtins and auto-install support.\"\"\"\n",
    "    # Auto-install any missing packages\n",
    "    auto_install_packages(code)\n",
    "\n",
    "    # If code uses matplotlib, inject Agg backend\n",
    "    if \"matplotlib\" in code or \"plt\" in code:\n",
    "        code = \"import matplotlib\\nmatplotlib.use('Agg')\\n\" + code\n",
    "\n",
    "    # Capture stdout\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = captured = io.StringIO()\n",
    "\n",
    "    try:\n",
    "        exec_globals = {\"__builtins__\": __builtins__}\n",
    "        exec(code, exec_globals)\n",
    "        output = captured.getvalue()\n",
    "        return output.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def ask_advanced(question):\n",
    "    \"\"\"Send a question to the LLM and execute code with full package support.\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=question,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=ADVANCED_SYSTEM_PROMPT,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    llm_text = response.text\n",
    "    print(\"LLM Response:\")\n",
    "    print(llm_text)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Extract code blocks\n",
    "    code_blocks = re.findall(\n",
    "        r\"<execute_python>(.*?)</execute_python>\", llm_text, re.DOTALL\n",
    "    )\n",
    "\n",
    "    if not code_blocks:\n",
    "        print(\"No code found in response.\")\n",
    "        return\n",
    "\n",
    "    for code in code_blocks:\n",
    "        code = code.strip()\n",
    "        result = advanced_execute_code(code)\n",
    "        print(f\"Execution Output:\\n{result}\")\n",
    "\n",
    "        # If a plot was saved, display it\n",
    "        if os.path.exists(\"_output.png\"):\n",
    "            from IPython.display import display, Image\n",
    "            display(Image(filename=\"_output.png\"))\n",
    "            os.remove(\"_output.png\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Advanced code execution pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scenario: Data Analysis with Pandas\n",
    "\n",
    "Let's ask the LLM to create and analyze a dataset using **pandas** — the most popular Python data analysis library.\n",
    "\n",
    "The advanced executor will automatically detect `import pandas` and install it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2.1: Pandas data analysis\n",
    "print(\"SCENARIO 2.1: Data Analysis with Pandas\")\n",
    "print(\"=\" * 50)\n",
    "ask_advanced(\n",
    "    \"Create a pandas DataFrame with columns Name, Age, Salary, Department for 8 employees. \"\n",
    "    \"Then compute: average salary by department, the oldest employee, and sort by salary descending. \"\n",
    "    \"Print all results.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scenario: Visualization with Matplotlib\n",
    "\n",
    "Now let's ask the LLM to generate a **plot**. The advanced executor handles this by:\n",
    "1. Injecting `matplotlib.use('Agg')` for non-interactive rendering\n",
    "2. The LLM saves the plot to `_output.png`\n",
    "3. We display it inline with `IPython.display.Image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2.2: Matplotlib visualization\n",
    "print(\"SCENARIO 2.2: Visualization with Matplotlib\")\n",
    "print(\"=\" * 50)\n",
    "ask_advanced(\n",
    "    \"Using numpy and matplotlib, generate 100 random data points from a normal distribution \"\n",
    "    \"(mean=0, std=1). Plot a histogram with 20 bins. Add a title 'Normal Distribution Histogram' \"\n",
    "    \"and axis labels. Save the plot with plt.savefig('_output.png') then call plt.close(). \"\n",
    "    \"Also print summary statistics: mean, std, min, max of the generated data.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scenario: Web Data Fetching with Requests\n",
    "\n",
    "The LLM can also write code that fetches **live data from the web**. Here we'll hit a public JSON API and format the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2.3: Web data fetching with requests\n",
    "print(\"SCENARIO 2.3: Web Data Fetching with Requests\")\n",
    "print(\"=\" * 50)\n",
    "ask_advanced(\n",
    "    \"Using the requests library, fetch data from https://jsonplaceholder.typicode.com/users \"\n",
    "    \"and print a formatted table showing each user's name, email, and city. \"\n",
    "    \"Use string formatting to align the columns nicely.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Scenario: Text Processing with regex + collections\n",
    "\n",
    "Not every task needs external packages. Here we use **stdlib modules** (`re`, `collections`) through the advanced pipeline to do text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2.4: Text processing with stdlib\n",
    "print(\"SCENARIO 2.4: Text Processing with regex + collections\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "paragraph = (\n",
    "    \"Artificial intelligence is transforming the way we live and work. \"\n",
    "    \"Machine learning algorithms can analyze vast amounts of data. \"\n",
    "    \"Natural language processing enables computers to understand human language. \"\n",
    "    \"Deep learning has achieved remarkable results in image recognition. \"\n",
    "    \"The future of AI holds tremendous potential for solving complex problems.\"\n",
    ")\n",
    "\n",
    "ask_advanced(\n",
    "    f\"Given this paragraph: '{paragraph}'\\n\\n\"\n",
    "    \"Use Python to: count word frequencies, find the top 5 most common words, \"\n",
    "    \"count the number of sentences, and compute the average word length. \"\n",
    "    \"Use the collections module (Counter). Print all results clearly.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Fixed tools are limiting** — a calculator agent with only 4 operations can't handle square roots, factorials, or data analysis\n",
    "2. **Code execution is a universal tool** — instead of building N tools, give the LLM the ability to write and run code\n",
    "3. **The basic pattern is simple**: system prompt + `<execute_python>` tags + `exec()` with stdout capture\n",
    "4. **Advanced execution with auto-install** — detect imports, pip install missing packages, run with full builtins\n",
    "5. **Real-world scenarios** — pandas for data analysis, matplotlib for visualization, requests for web APIs, stdlib for text processing\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Feature | Basic Pipeline | Advanced Pipeline |\n",
    "|---|---|---|\n",
    "| Allowed packages | `math`, `statistics` only | Any Python package |\n",
    "| Package installation | Not supported | Auto-detect and install |\n",
    "| Globals | Restricted | Full `__builtins__` |\n",
    "| Visualization | Not supported | matplotlib with auto-save/display |\n",
    "| Use case | Simple math, stats | Data analysis, web, visualization |\n",
    "\n",
    "### Safety Considerations\n",
    "\n",
    "- **Never run LLM-generated code in production without sandboxing** — use Docker containers, E2B, or similar\n",
    "- **Review generated code** before execution when possible\n",
    "- **Restrict network access** if the code doesn't need it\n",
    "- **Set timeouts** to prevent infinite loops\n",
    "- **Use read-only file systems** to prevent data corruption\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore **Google's built-in code execution** tool in Gemini API\n",
    "- Look into sandboxed execution environments (Docker, E2B, etc.)\n",
    "- Combine code execution with other tools for a more capable agent\n",
    "- Add **error recovery** — if code fails, send the error back to the LLM for a retry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
